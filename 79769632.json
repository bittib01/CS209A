{
  "question": {
    "tags": [
      "java",
      "apache-spark",
      "amazon-s3",
      "hadoop"
    ],
    "owner": {
      "account_id": 7489950,
      "reputation": 5724,
      "user_id": 5692012,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://i.sstatic.net/a3FwR.gif?s=256",
      "display_name": "RobinFrcd",
      "link": "https://stackoverflow.com/users/5692012/robinfrcd"
    },
    "is_answered": true,
    "view_count": 160,
    "answer_count": 3,
    "score": 0,
    "last_activity_date": 1764526583,
    "creation_date": 1758292166,
    "last_edit_date": 1758367080,
    "question_id": 79769632,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79769632/how-to-connect-to-s3-without-the-large-aws-sdk-v2-bundle",
    "title": "How to connect to S3 without the large AWS SDK v2 bundle?",
    "body": "<p>I'm trying to read some file from S3 with PySpark 4.0.1 and the <code>S3AFileSystem</code>.</p>\n<p>The standard configuration using <a href=\"https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.4.1\" rel=\"nofollow noreferrer\">hadoop-aws 3.4.1</a> works, but it requires the <a href=\"https://mvnrepository.com/artifact/software.amazon.awssdk/bundle/2.24.6\" rel=\"nofollow noreferrer\">AWS SDK Bundle</a>. This single dependency is over 530 MB, which significantly increases the size of my application's deployment.</p>\n<p>My goal is to find a minimal set of AWS SDK v2 dependencies required by the <code>hadoop-aws</code> library to connect to S3, without including the entire bundle. I'm aware of the <a href=\"https://issues.apache.org/jira/browse/SPARK-49508\" rel=\"nofollow noreferrer\">open issue on Spark's Jira</a>, but there is no resolution yet.</p>\n<p>I've tried to include <code>com.amazonaws:aws-java-sdk-s3</code> only, and others sub-dependencies but I always hit:</p>\n<pre class=\"lang-java prettyprint-override\"><code>Py4JJavaError: An error occurred while calling o83.parquet.\n: java.lang.NoClassDefFoundError:\nsoftware/amazon/awssdk/utils/builder/ToCopyableBuilder\n</code></pre>\n<p>Has anyone successfully identified the minimum set of artifacts needed for a basic s3a:// read/write operation with Spark?</p>\n"
  },
  "answers": [
    {
      "owner": {
        "account_id": 2611206,
        "reputation": 13615,
        "user_id": 2261274,
        "user_type": "registered",
        "profile_image": "https://i.sstatic.net/10mj1.jpg?s=256",
        "display_name": "stevel",
        "link": "https://stackoverflow.com/users/2261274/stevel"
      },
      "is_accepted": false,
      "score": 0,
      "last_activity_date": 1764526583,
      "creation_date": 1764526583,
      "answer_id": 79834104,
      "question_id": 79769632,
      "content_license": "CC BY-SA 4.0",
      "body": "<p>IT can be done: in fact if for some reason there's an explicit use of a shaded dependency: file a HADOOP jira on the ASF jira server.</p>\n<p>If all you want to do is work with s3, you need the s3 module from the software.amazon.awssdk project (not the v1 com.amazonaws project, which is what burned you)</p>\n<p><a href=\"https://mvnrepository.com/artifact/software.amazon.awssdk/s3/2.24.6\" rel=\"nofollow noreferrer\">https://mvnrepository.com/artifact/software.amazon.awssdk/s3/2.24.6</a></p>\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;dependency org=&quot;software.amazon.awssdk&quot; name=&quot;s3-transfer-manager&quot; rev=&quot;2.24.6&quot;/&gt;\n&lt;dependency org=&quot;software.amazon.awssdk&quot; name=&quot;s3&quot; rev=&quot;2.24.6&quot;/&gt;\n</code></pre>\n<p>This will pull in all the dependencies to talk to s3.</p>\n<p>If you need to talk to STS (issuing session credentials), pull in the matching &quot;sts&quot; module.</p>\n<p>you <em>should</em> use the exact version of the sdk hadoop was built with. You <em>must not</em> use an older version. You <em>may</em> use a later one -but there's a risk it won't work, especially with third party stores, which amazon broke compatibility with.</p>\n<p>If you are wondering why hadoop and spark builds ship with bundle.jar, its because that includes all the dependencies shaded so as not to cause conflict with any other version on your classpath. Once you go to individual modules, you are on your own.</p>\n"
    },
    {
      "owner": {
        "account_id": 231844,
        "reputation": 17955,
        "user_id": 496289,
        "user_type": "registered",
        "accept_rate": 86,
        "profile_image": "https://i.sstatic.net/MBIsC.jpg?s=256",
        "display_name": "Kashyap",
        "link": "https://stackoverflow.com/users/496289/kashyap"
      },
      "is_accepted": false,
      "score": 1,
      "last_activity_date": 1758837884,
      "creation_date": 1758837884,
      "answer_id": 79775338,
      "question_id": 79769632,
      "content_license": "CC BY-SA 4.0",
      "body": "<ul>\n<li><p>If you want to use `s3a` then you have no choice. `hadoop-aws` has a hard dependency on aws-sdk so can't avoid.</p>\n</li>\n<li><p>You can spend few months writing your own extension to support a new `s3robin` and in it's implementation use the AWS REST API instead of SDK.</p>\n</li>\n</ul>\n<p><em>You have mentioned nothing of the environment where you deploy or how you package your application. But you could:</em></p>\n<ul>\n<li>Change the way you deploy. E.g. you could add these as dependencies to your cluster's config (e.g. databricks cluster created using terraform), or your spark builder where you create your spark session. This way you don't have to &quot;package&quot; the sdk, instead spark will download it from mvn central when cluster/spark starts.</li>\n</ul>\n"
    },
    {
      "owner": {
        "account_id": 4013257,
        "reputation": 2242,
        "user_id": 3306099,
        "user_type": "registered",
        "profile_image": "https://www.gravatar.com/avatar/d7c32f4e0b88c7044d31b1c087ebd5d1?s=256&d=identicon&r=PG&f=y&so-version=2",
        "display_name": "rjrudin",
        "link": "https://stackoverflow.com/users/3306099/rjrudin"
      },
      "is_accepted": false,
      "score": 0,
      "last_activity_date": 1758649908,
      "creation_date": 1758649908,
      "answer_id": 79772928,
      "question_id": 79769632,
      "content_license": "CC BY-SA 4.0",
      "body": "<p>I can't guarantee this works in all scenarios, but at least for some basic read/write tests between Spark 4.0.1 and S3, including <code>software.amazon.awssdk:s3-transfer-manager:2.24.6</code> does the trick. I am using the following Gradle config in my project; adjust as necessary for your environment:</p>\n<pre><code>implementation(&quot;org.apache.hadoop:hadoop-aws:3.4.1&quot;) {\n  exclude group: &quot;software.amazon.awssdk&quot;\n}\nimplementation &quot;software.amazon.awssdk:s3-transfer-manager:2.24.6&quot;\n</code></pre>\n"
    }
  ],
  "question_comments": [
    {
      "owner": {
        "account_id": 372682,
        "reputation": 26512,
        "user_id": 721855,
        "user_type": "registered",
        "profile_image": "https://i.sstatic.net/vZiox.png?s=256",
        "display_name": "aled",
        "link": "https://stackoverflow.com/users/721855/aled"
      },
      "edited": false,
      "score": 0,
      "creation_date": 1758380188,
      "post_id": 79769632,
      "comment_id": 140749192,
      "content_license": "CC BY-SA 4.0"
    },
    {
      "owner": {
        "account_id": 12987761,
        "reputation": 634,
        "user_id": 9388056,
        "user_type": "registered",
        "profile_image": "https://i.sstatic.net/lwIgN.jpg?s=256",
        "display_name": "Frank",
        "link": "https://stackoverflow.com/users/9388056/frank"
      },
      "edited": false,
      "score": 0,
      "creation_date": 1758297744,
      "post_id": 79769632,
      "comment_id": 140747689,
      "content_license": "CC BY-SA 4.0"
    }
  ],
  "answer_comments": {
    "79834104": [
      {
        "owner": {
          "account_id": 7489950,
          "reputation": 5724,
          "user_id": 5692012,
          "user_type": "registered",
          "accept_rate": 50,
          "profile_image": "https://i.sstatic.net/a3FwR.gif?s=256",
          "display_name": "RobinFrcd",
          "link": "https://stackoverflow.com/users/5692012/robinfrcd"
        },
        "edited": false,
        "score": 0,
        "creation_date": 1764606539,
        "post_id": 79834104,
        "comment_id": 140881764,
        "content_license": "CC BY-SA 4.0"
      }
    ],
    "79775338": [
      {
        "owner": {
          "account_id": 231844,
          "reputation": 17955,
          "user_id": 496289,
          "user_type": "registered",
          "accept_rate": 86,
          "profile_image": "https://i.sstatic.net/MBIsC.jpg?s=256",
          "display_name": "Kashyap",
          "link": "https://stackoverflow.com/users/496289/kashyap"
        },
        "reply_to_user": {
          "account_id": 7489950,
          "reputation": 5724,
          "user_id": 5692012,
          "user_type": "registered",
          "accept_rate": 50,
          "profile_image": "https://i.sstatic.net/a3FwR.gif?s=256",
          "display_name": "RobinFrcd",
          "link": "https://stackoverflow.com/users/5692012/robinfrcd"
        },
        "edited": false,
        "score": 1,
        "creation_date": 1758892517,
        "post_id": 79775338,
        "comment_id": 140761866,
        "content_license": "CC BY-SA 4.0"
      },
      {
        "owner": {
          "account_id": 7489950,
          "reputation": 5724,
          "user_id": 5692012,
          "user_type": "registered",
          "accept_rate": 50,
          "profile_image": "https://i.sstatic.net/a3FwR.gif?s=256",
          "display_name": "RobinFrcd",
          "link": "https://stackoverflow.com/users/5692012/robinfrcd"
        },
        "edited": false,
        "score": 0,
        "creation_date": 1758889698,
        "post_id": 79775338,
        "comment_id": 140761790,
        "content_license": "CC BY-SA 4.0"
      }
    ],
    "79772928": []
  }
}