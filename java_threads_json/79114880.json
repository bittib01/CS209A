{
  "question": {
    "tags": [
      "java",
      "apache-spark",
      "pyspark",
      "apache-spark-sql"
    ],
    "owner": {
      "account_id": 438144,
      "reputation": 1377,
      "user_id": 826235,
      "user_type": "registered",
      "accept_rate": 32,
      "profile_image": "https://www.gravatar.com/avatar/8d7ab39ed79ff88d1dfede4d188715e8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "yogi",
      "link": "https://stackoverflow.com/users/826235/yogi"
    },
    "is_answered": false,
    "view_count": 104,
    "answer_count": 1,
    "score": 0,
    "last_activity_date": 1729637854,
    "creation_date": 1729611739,
    "question_id": 79114880,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79114880/reading-spark-defaults-conf-when-using-spark-rest-http-server-with-a-java-applic",
    "title": "Reading spark-defaults.conf when using Spark REST HTTP server with a Java application",
    "body": "<p>I'm using the Spark rest server to submit jobs. When submitting a pyspark application, the docs set the <code>mainClass</code> to be <code>org.apache.spark.deploy.SparkSubmit</code>, and then pass the python script as the <code>appArgs</code>. This means that the application is being run with <code>SparkSubmit</code>, which by default reads the <code>spark-defaults.conf</code>. For example:</p>\n<pre class=\"lang-bash prettyprint-override\"><code>curl -X POST http://localhost:6066/v1/submissions/create --header &quot;Content-Type:application/json;charset=UTF-8&quot; --data '{\n&quot;action&quot; : &quot;CreateSubmissionRequest&quot;,\n&quot;appArgs&quot; : [ &quot;s3n://&lt;bucket-Name&gt;/pi.py&quot; ],\n&quot;appResource&quot; : &quot;s3n://&lt;bucket-Name&gt;/pi.py&quot;,\n&quot;environmentVariables&quot; : {\n    &quot;SPARK_ENV_LOADED&quot; : &quot;1&quot;\n},\n&quot;mainClass&quot; : &quot;org.apache.spark.deploy.SparkSubmit&quot;,\n&quot;sparkProperties&quot; : {\n&quot;spark.driver.supervise&quot; : &quot;false&quot;,\n&quot;spark.app.name&quot; : &quot;Simple App&quot;,\n&quot;spark.eventLog.enabled&quot;: &quot;true&quot;,\n&quot;spark.submit.deployMode&quot; : &quot;cluster&quot;,\n&quot;spark.master&quot; : &quot;spark://localhost:6066&quot;\n}\n}'\n</code></pre>\n<p>I'm trying to achieve the same behaviour with a Java spark job instead of Python. Most of the examples put the Job class itself as the <code>mainClass</code>, which means it's not being run with SparkSubmit, and the job does not have the configuration from <code>spark-defaults.conf</code>.</p>\n<p>I could not find a way to use <code>SparkSubmit</code> with a jar file, since there is no option to supply the <code>--class</code> argument of SparkSubmit.</p>\n<p>How do I specify the <code>--class</code> when submitting using the API, or alternatively make the class load the spark-defaults.conf in a different way?</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}