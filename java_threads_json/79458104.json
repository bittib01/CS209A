{
  "question": {
    "tags": [
      "java",
      "object-detection",
      "yolo",
      "onnx",
      "yolov5"
    ],
    "owner": {
      "account_id": 27578716,
      "reputation": 23,
      "user_id": 21048361,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/a/AEdFTp6dxiFZGBeYSmKIvMTaOiG_WniDLeH3QCCJr9aR=k-s256",
      "display_name": "CoffeeCoding",
      "link": "https://stackoverflow.com/users/21048361/coffeecoding"
    },
    "is_answered": true,
    "view_count": 270,
    "accepted_answer_id": 79459171,
    "answer_count": 2,
    "score": 0,
    "last_activity_date": 1740344917,
    "creation_date": 1740156435,
    "question_id": 79458104,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79458104/issue-with-object-detection-results-in-java-using-yolov5-onnx-model",
    "title": "Issue with Object Detection Results in Java using YOLOv5 ONNX Model",
    "body": "<p>I trained a neural network for object detection using YOLOv5 and exported it to the ONNX format, as I need to use it in a Java application. However, I am encountering issues with incorrect class IDs and bounding boxes in my Java code, while the Python implementation works as expected.</p>\n<p>Issues Encountered:</p>\n<p>Incorrect Class IDs: All of the detections in the Java code are returning class ID 0, which is not expected. In the Python code, I receive valid class IDs corresponding to the detected objects.</p>\n<p>Bounding Box Offsets: The bounding boxes appear to have a symmetrical offset, being shifted too far positively on the x-axis and too far down on the y-axis. This results in inaccurate positioning of the detected objects in the output image.</p>\n<p>Wrong interpretation of Model Output?</p>\n<p>(NMS not yet implemented in JAVA-Code)</p>\n<p>For Reference here are some infos about the model:</p>\n<pre><code>Model Properties:\nFormat: ONNX v8\nProducer: PyTorch 2.6.0\nVersion: 0\nImports: ai.onnx v17\nGraph: main_graph\n\nMetadata:\nStride: 32\nClass Names: {0: '1', 1: '2', 2: '3', 3: '4', 4: '5', 5: '6', 6: '7', 7: '8', 8: '9', 9: '10'}\nInputs:\n\nName: images\nTensor: float32\\[1,3,640,640\\] (that means 1 image, RGB, dimensions (640,640))\nOutputs:\n\nName: output0\nTensor: float32\\[1,25200,15\\] (not fully sure what this means)\n</code></pre>\n<p>Now to the code(the same test image is used in both cases):</p>\n<p>This is the working python code that gives the expected results:</p>\n<pre><code># !pip install torch torchvision pillow\n\nimport torch\nfrom PIL import Image\n\n# Path to the YOLOv5 repository\npath_to_yolo_library = '/content/yolov5'  \nonnx_path = '/content/best.onnx'  \nimage_path = '/content/img101.png'  \n\n# Import the YOLOv5 model from the local path\nmodel = torch.hub.load(path_to_yolo_library, 'custom', path=onnx_path, source='local') \n\n# Load and preprocess the image\nimg = Image.open(image_path)  # Load image as a PIL image\nimg = img.resize((640, 640))  # Resize the image to fit YOLO input size (640x640)\n\n# Inference (includes NMS)\nresults = model(img, size=640)  # Inference with NMS\n\n# Results\nresults.print()  # Print the results (detections, classes, confidence)\nresults.show()   # Show the image with bounding boxes\nresults.save()   # Save the result images\n\n# Data: Print the bounding boxes, confidence scores, and class ids\nprint('\\n', results.xyxy[0])  # Print predictions in the format (x1, y1, x2, y2, confidence, class)\n</code></pre>\n<p>Output:\n(includes correct visualization)</p>\n<pre><code>image 1/1: 640x640 2 1s, 2 2s, 1 3, 2 4s, 1 5, 1 6, 1 7\nSpeed: 16.7ms pre-process, 407.8ms inference, 6.0ms NMS per image at shape (1, 3, 640, 640)\ntensor([[4.49145e+01, 1.94186e+02, 1.14293e+02, 3.11326e+02, 8.03208e-01, 0.00000e+00],\n        [4.44819e+01, 3.47444e+02, 1.18352e+02, 4.73753e+02, 7.96138e-01, 1.00000e+00],\n        [3.68868e+02, 2.70193e+01, 4.38986e+02, 1.55611e+02, 7.92952e-01, 1.00000e+00],\n        [4.62871e+01, 3.24609e+01, 1.15780e+02, 1.50192e+02, 7.83159e-01, 0.00000e+00],\n        [3.47603e+02, 4.95154e+02, 4.30347e+02, 6.35069e+02, 7.63681e-01, 4.00000e+00],....\n</code></pre>\n<p>Since this worked very well and accurately i tried translating it into java:</p>\n<pre><code>import java.awt.BasicStroke;\nimport java.awt.Color;\nimport java.awt.Graphics;\nimport java.awt.image.BufferedImage;\nimport java.io.File;\nimport java.nio.FloatBuffer;\nimport java.util.Collections;\nimport java.util.Map;\n\nimport javax.imageio.ImageIO;\n\nimport ai.onnxruntime.OnnxTensor;\nimport ai.onnxruntime.OrtEnvironment;\nimport ai.onnxruntime.OrtSession;\n\npublic class YOLOv5ONNXJava {\n    public static void main(String[] args) {\n        try {\n            // Load ONNX model\n            String modelPath = &quot;...best.onnx&quot;;\n            OrtEnvironment env = OrtEnvironment.getEnvironment();\n            OrtSession session = env.createSession(modelPath, new OrtSession.SessionOptions());\n\n            // Load Image\n            BufferedImage image = ImageIO.read(new File(&quot;...img101.png&quot;));\n            int origWidth = image.getWidth();\n            int origHeight = image.getHeight();\n            int inputSize = 640;\n\n            BufferedImage resizedImage = resizeImage(image, inputSize, inputSize);\n\n            // Convert Image to Tensor\n            float[] inputTensor = preprocessImage(resizedImage, inputSize);\n            long[] shape = {1, 3, inputSize, inputSize}; // Batch size 1, RGB channels\n            OnnxTensor tensor = OnnxTensor.createTensor(env, FloatBuffer.wrap(inputTensor), shape);\n\n            // Run inference\n            Map&lt;String, OnnxTensor&gt; inputMap = Collections.singletonMap(session.getInputNames().iterator().next(), tensor);\n            OrtSession.Result result = session.run(inputMap);\n\n            // Process Output\n            float[][][] outputData = (float[][][]) result.get(0).getValue();\n            float[][] detections = outputData[0]; // Extract first batch\n\n            // Post-process detections and draw rectangles\n            postProcess(detections, origWidth, origHeight, image);\n\n            // Cleanup\n            session.close();\n            tensor.close();\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    // Resize image function\n    public static BufferedImage resizeImage(BufferedImage originalImage, int width, int height) {\n        BufferedImage resizedImage = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB);\n        resizedImage.getGraphics().drawImage(originalImage, 0, 0, width, height, null);\n        return resizedImage;\n    }\n\n    // Preprocess image into tensor format\n    public static float[] preprocessImage(BufferedImage image, int inputSize) {\n        float[] tensor = new float[3 * inputSize * inputSize]; // RGB channels\n        int[] rgbArray = image.getRGB(0, 0, inputSize, inputSize, null, 0, inputSize);\n\n        for (int i = 0; i &lt; inputSize * inputSize; i++) {\n            int pixel = rgbArray[i];\n            tensor[i] = ((pixel &gt;&gt; 16) &amp; 0xFF) / 255.0f; // Red\n            tensor[i + inputSize * inputSize] = ((pixel &gt;&gt; 8) &amp; 0xFF) / 255.0f; // Green\n            tensor[i + 2 * inputSize * inputSize] = (pixel &amp; 0xFF) / 255.0f; // Blue\n        }\n        return tensor;\n    }\n\n // Updated postProcess method to draw rectangles and labels\n    public static void postProcess(float[][] detections, int origWidth, int origHeight, BufferedImage originalImage) {\n        System.out.println(&quot;\\nDetections:&quot;);\n\n        int inputSize = 640; // YOLOv5 default input size\n        float gain = Math.min((float) inputSize / origWidth, (float) inputSize / origHeight);\n        float padX = (inputSize - origWidth * gain) / 2;\n        float padY = (inputSize - origHeight * gain) / 2;\n\n        // Create a copy of the original image to draw on\n        BufferedImage outputImage = new BufferedImage(origWidth, origHeight, BufferedImage.TYPE_INT_RGB);\n        Graphics g = outputImage.getGraphics();\n        g.drawImage(originalImage, 0, 0, null);\n\n        for (float[] row : detections) {\n            if (row.length &lt; 6) continue;\n\n            // Extract bounding box values\n            float x1 = row[0], y1 = row[1], x2 = row[2], y2 = row[3];\n            float confidence = row[4];\n            int classId = (int) row[5]; // Extract raw class ID\n\n            // Apply YOLOv5 scaling transformation\n           x1 = (x1 - padX) / gain; // Adjust x1\n           y1 = (y1 - padY) / gain; // Adjust y1\n            x2 = (x2 - padX) / gain; // Adjust x2\n            y2 = (y2 - padY) / gain; // Adjust y2\n\n            // Clip bounding boxes to image boundaries\n           \n\n            if (confidence &gt; 0.5) {\n                System.out.printf(&quot;BBox: [%.2f, %.2f, %.2f, %.2f], Confidence: %.2f, Class ID: %d%n&quot;,\n                        x1, y1, x2, y2, confidence, classId);\n                \n                // Draw the bounding box\n                g.setColor(Color.RED); // Set color for bounding box\n                g.drawRect((int) x1, (int)y1,(int)x2,(int)y2);\n                \n                // Draw the label with confidence score\n                g.setColor(Color.WHITE);\n                g.drawString(String.format(&quot;ID: %d Conf: %.2f&quot;, classId, confidence), (int) Math.round(x1), (int) Math.round(y1 - 10));\n            }\n        }\n        g.dispose();\n\n        // Save the output image\n        try {\n            ImageIO.write(outputImage, &quot;jpg&quot;, new File(&quot;output.jpg&quot;));\n            System.out.println(&quot;Output image saved as output.jpg&quot;);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n}\n</code></pre>\n<pre><code>Output: \n(rectangles have a wrong offset and i dont know why the class ID are 0 everywhere)\nBBox: [79,97, 92,82, 56,06, 119,68], Confidence: 0,70, Class ID: 0\nBBox: [81,12, 92,92, 57,66, 115,62], Confidence: 0,76, Class ID: 0\nBBox: [405,68, 88,88, 56,72, 119,84], Confidence: 0,67, Class ID: 0\nBBox: [81,99, 94,62, 61,15, 112,69], Confidence: 0,51, Class ID: 0\n</code></pre>\n<p>Conclusion:\nI am looking for guidance on how to resolve these discrepancies between the Python and Java implementations. Any insights on adjusting the preprocessing steps, interpreting the ONNX model output, or debugging the bounding box coordinates would be greatly appreciated.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}