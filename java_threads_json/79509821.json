{
  "question": {
    "tags": [
      "java",
      "apache-spark",
      "memory-management",
      "apache-spark-sql",
      "memory-leaks"
    ],
    "owner": {
      "account_id": 6540273,
      "reputation": 215,
      "user_id": 5058138,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://www.gravatar.com/avatar/17525e86081432045752cec8d74d4750?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "codeluv",
      "link": "https://stackoverflow.com/users/5058138/codeluv"
    },
    "is_answered": true,
    "view_count": 234,
    "answer_count": 1,
    "score": 3,
    "last_activity_date": 1743602658,
    "creation_date": 1741973082,
    "question_id": 79509821,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79509821/memory-increasing-monotonically-in-spark-job",
    "title": "Memory increasing monotonically in Spark job",
    "body": "<p>I need some help with a spark memory issue. I have my Spark application running inside a single JVM as a Kubernetes pod. The nature of the job is that it is a batched job, it runs for 4-5 hours and then the pod is idle. Each batch is processed after the 12-hour gap.</p>\n<p><strong>Issue</strong>: The pod's memory keeps increasing during this 4-5 hours job cycle and once all the jobs are done in the batch, and pod is idle, the memory does not get lowered.</p>\n<p>Let's say I have allocated 100GB to a pod, it increases to 80GB during the batch cycle, and once all the jobs are completed, and the pod is idle, the memory does not come down from 80 GB. In the next batch cycle (the gap is 12 hours), memory starts increasing from 80GB until it gets OOM killed at 100GB</p>\n<p>I tried taking a heap dump as well. The heap dump points to an unsafe Memory allocator. I suspect that physical memory is getting allocated outside JVM and not cleaned up, but I'm unsure what Spark operation/configuration can cause this issue.</p>\n<p>Also, in case it is allocated outside the heap, why it is not getting cleaned up?</p>\n<pre><code>One instance of org.apache.spark.unsafe.memory.HeapMemoryAllocator loaded by jdk.internal.loader.ClassLoaders$AppClassLoader 1,61,06,14,312 (89.24%) bytes. The memory is accumulated in one instance of java.util.LinkedList, loaded by &lt;system class loader&gt;, which occupies 1,61,06,14,112 (89.24%) bytes.\n</code></pre>\n<p>Also, I checked in the &quot;Storage&quot; tab of the Spark UI as well, it doesn't show any RDD cached.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}