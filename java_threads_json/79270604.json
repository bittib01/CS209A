{
  "question": {
    "tags": [
      "java",
      "amazon-web-services",
      "apache-spark",
      "pyspark",
      "jdbc"
    ],
    "owner": {
      "account_id": 15410372,
      "reputation": 2759,
      "user_id": 11117255,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/af6e7bb7e0e01200486e1a1c4d8cdf7c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Cauder",
      "link": "https://stackoverflow.com/users/11117255/cauder"
    },
    "is_answered": false,
    "view_count": 235,
    "answer_count": 1,
    "score": 1,
    "last_activity_date": 1734946278,
    "creation_date": 1733896921,
    "question_id": 79270604,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79270604/spark-pyspark-redshift-jdbc-write-no-suitable-driver-classnotfoundexception",
    "title": "“Spark-PySpark Redshift JDBC Write: No suitable driver / ClassNotFoundException: com.amazon.redshift.jdbc42.Driver Errors”",
    "body": "<p>I’m trying to write a DataFrame from Spark (PySpark) to an Amazon Redshift Serverless cluster using the Redshift JDBC driver.</p>\n<p>I keep running into driver-related errors:</p>\n<pre><code>•   java.sql.SQLException: No suitable driver\n\n•   java.lang.ClassNotFoundException: com.amazon.redshift.jdbc42.Driver\n</code></pre>\n<p>What I’ve Tried:</p>\n<pre><code>1.  Setup:\n\n•   Spark version: (e.g., Spark 3.3.1)\n\n•   Hadoop AWS packages: --packages org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901\n\n•   Redshift JDBC driver: RedshiftJDBC42-2.1.0.30.jar downloaded from Amazon’s official site.\n\n2.  spark-submit command:\n</code></pre>\n<p>spark-submit <br />\n--conf spark.driver.bindAddress=127.0.0.1 <br />\n--conf spark.driver.host=127.0.0.1 <br />\n--driver-memory 4g <br />\n--packages org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901 <br />\n--jars /path/to/RedshiftJDBC42-2.1.0.30.jar <br />\n--driver-class-path /path/to/RedshiftJDBC42-2.1.0.30.jar <br />\nmy_script.py</p>\n<p>I’ve tried adding --driver-class-path so that the driver is visible to the driver. The JAR file definitely exists at the specified path.</p>\n<pre><code>3.  In the Python Code:\n</code></pre>\n<p>jdbc_url = &quot;jdbc:redshift://:5439/dev&quot;\n(df.write\n.format(&quot;jdbc&quot;)\n.option(&quot;url&quot;, jdbc_url)\n.option(&quot;dbtable&quot;, &quot;public.my_staging_table&quot;)\n.option(&quot;user&quot;, os.environ[&quot;REDSHIFT_USER&quot;])\n.option(&quot;password&quot;, os.environ[&quot;REDSHIFT_PASSWORD&quot;])\n.option(&quot;driver&quot;, &quot;com.amazon.redshift.jdbc42.Driver&quot;)\n.mode(&quot;append&quot;)\n.save())</p>\n<p>The code runs fine until the .save() step, at which point I get No suitable driver or a ClassNotFoundException for the Redshift driver class.</p>\n<p>What I Know:</p>\n<pre><code>•   The Redshift JDBC driver class should be com.amazon.redshift.jdbc42.Driver.\n\n•   I’ve seen suggestions to use --driver-class-path plus --jars to ensure the driver is on both driver and executor classpaths.\n\n•   If I remove --driver-class-path, I sometimes get ClassNotFoundException. With it, I still get No suitable driver.\n\n•   My AWS credentials and S3 reading works fine (I can read JSON from S3). The problem occurs only at the JDBC write to Redshift step.\n</code></pre>\n<p>Questions:</p>\n<pre><code>•   Is there another configuration step needed to ensure Spark recognizes the Redshift driver?\n\n•   Do I need to specify any additional spark configs for the JDBC driver?\n\n•   Are there known compatibility issues with this Redshift driver version and Spark/Hadoop versions?\n\n•   Should I consider a different Redshift driver JAR or a different approach (like spark-redshift or redshift-jdbc42-no-awssdk JAR)?\n</code></pre>\n<p>Any guidance on resolving the No suitable driver and ClassNotFoundException errors when writing to Redshift via JDBC in Spark would be greatly appreciated.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}