{
  "question": {
    "tags": [
      "java",
      "scala",
      "apache-spark",
      "pyspark",
      "databricks"
    ],
    "owner": {
      "account_id": 17598288,
      "reputation": 3,
      "user_id": 12768801,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/5TPN9.jpg?s=256",
      "display_name": "Paul Douane",
      "link": "https://stackoverflow.com/users/12768801/paul-douane"
    },
    "is_answered": false,
    "view_count": 78,
    "answer_count": 0,
    "score": 0,
    "last_activity_date": 1750291290,
    "creation_date": 1750291290,
    "question_id": 79671409,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79671409/unable-to-load-org-apache-spark-sql-delta-classes-from-jvm-pyspark",
    "title": "Unable to load org.apache.spark.sql.delta classes from JVM pyspark",
    "body": "<p>I’m working on Databricks with a cluster running Runtime 16.4, which includes Spark 3.5.2 and Scala 2.12.</p>\n<p>For a specific need, I want to implement my own custom way of writing to Delta tables by manually managing Delta transactions from PySpark. To do this, I want to access the Delta Lake transactional engine via the JVM embedded in the Spark session, specifically by using the class:</p>\n<pre><code>org.apache.spark.sql.delta.DeltaLog\n</code></pre>\n<p><strong>Issue</strong></p>\n<p>When I try to use classes from the package org.apache.spark.sql.delta directly from PySpark (through spark._jvm), the classes are not found if I don’t have the Delta Core package installed explicitly on the cluster.</p>\n<p>When I install the Delta Core Python package to gain access, I encounter the following Python import error:</p>\n<pre><code>ModuleNotFoundError: No module named 'delta.exceptions.captured'; 'delta.exceptions' is not a package\n</code></pre>\n<p>Without the Delta Core package installed, accessing DeltaLog simply returns a generic JavaPackage object that is unusable.</p>\n<p>What I want to do\nAccess the Delta transaction log API (DeltaLog) from PySpark via JVM.</p>\n<p>Be able to start transactions and commit manually to implement custom write behavior.</p>\n<p>Work within the Databricks Runtime 16.4 environment without conflicts or missing dependencies.</p>\n<p><strong>Questions</strong></p>\n<p>How can I correctly access and use <code>org.apache.spark.sql.delta.DeltaLog</code> from PySpark on Databricks Runtime 16.4?</p>\n<p>Is there a supported way to manually manage Delta transactions through the JVM in this environment?</p>\n<p>What is the correct setup or package dependency to avoid the <code>ModuleNotFoundError</code> when installing the Delta Core Python package?</p>\n<p>Are there any alternatives or recommended patterns to achieve manual Delta commits programmatically on Databricks?</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}