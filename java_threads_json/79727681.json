{
  "question": {
    "tags": [
      "java",
      "memory",
      "external-sorting"
    ],
    "owner": {
      "account_id": 709632,
      "reputation": 1053,
      "user_id": 620054,
      "user_type": "registered",
      "accept_rate": 70,
      "profile_image": "https://www.gravatar.com/avatar/b9980ecabf192a4b53ff1d541a7b5229?s=256&d=identicon&r=PG",
      "display_name": "Michael K",
      "link": "https://stackoverflow.com/users/620054/michael-k"
    },
    "is_answered": true,
    "view_count": 270,
    "accepted_answer_id": 79741668,
    "answer_count": 2,
    "score": 0,
    "last_activity_date": 1756911788,
    "creation_date": 1754503098,
    "last_edit_date": 1754986042,
    "question_id": 79727681,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79727681/combining-two-huge-text-files-and-removing-duplicates",
    "title": "Combining two huge text files and removing duplicates",
    "body": "<h2>The Specific Problem</h2>\n<p>I am starting with two gigantic input text files containing lots of lines with a single string value on each line. I'd like to write a Java program to combine the contents of those two files into one output text file, while also removing any duplicates. <strong>What method would be best for performance?</strong></p>\n<h2>What data sizes are we talking about?</h2>\n<p>The combined file might be <strong>250 GB, maybe larger</strong>. I did a back-of-the-napkin calculation using a smaller example file and the 250 GB file may have around <strong>27 billion lines</strong> (very rough) if it's proportional.</p>\n<p>So we're definitely running into the territory of max Integer value in Java (2,147,483,647). We can imagine that the value on each line is a max of 20 characters, otherwise it will be discarded.</p>\n<p>And let's assume the input files and output file (and likely any intermediate data structure used for duplicate checks) will be too large for memory. I will have the use of a computer with a large amount of memory, but it may not be enough.</p>\n<p>Let me also note that the average length of a line is probably 7-10 characters. And I will be discarding any lines that are longer than 25 characters. I mention this because hashing was brought up as a method for duplicate checking. It seems the hash for these values would take up more space than the actual value.</p>\n<h2>Planning</h2>\n<p>Initially I was thinking of how to do the duplicate check to reduce processing time, as I'm sure I will want to make it run as fast as possible. However I realize the more immediate problem is how to deal with the large sizes and whatever I use to check for duplicates or sort will not fit in memory. Possibly sort the input files first, then merge together and discard duplicates in one pass. The output file can be sorted, doesn't need to maintain original order.</p>\n<h2>I've also been doing my research.</h2>\n<p>I came across this previous question-\n<a href=\"https://stackoverflow.com/questions/52315045/removing-duplicate-strings-from-huge-text-files\">Removing duplicate strings from huge text files</a></p>\n<p>The only answer mostly deals with data sizes that would fit into memory. But at the end it mentions a possible way to do this with a large file (just one file) that doesn't fit into RAM. I'm not sure I fully understand it, and it would seem that the duplicate checking mechanism itself would also be very large and not fit into memory contrary to what the author intended.</p>\n<p>And I read up on External Sorting.\n<a href=\"https://en.wikipedia.org/wiki/External_sorting\" rel=\"nofollow noreferrer\">https://en.wikipedia.org/wiki/External_sorting</a></p>\n<p>That could definitely be something I try if I implement it with sorting first.</p>\n<p>What method do you suggest? Many thanks for input!</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}