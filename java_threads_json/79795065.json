{
  "question": {
    "tags": [
      "java",
      "spring",
      "spring-batch"
    ],
    "owner": {
      "account_id": 13964222,
      "reputation": 858,
      "user_id": 10084505,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7d1cf77a5c15cd178875d5220fec8157?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Walnussb&#228;r",
      "link": "https://stackoverflow.com/users/10084505/walnussb%c3%a4r"
    },
    "is_answered": true,
    "view_count": 56,
    "accepted_answer_id": 79795216,
    "answer_count": 1,
    "score": 0,
    "last_activity_date": 1760988368,
    "creation_date": 1760975918,
    "question_id": 79795065,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79795065/spring-batch-write-multiple-chunks-at-once-aggregate-writing-of-chunks-after-re",
    "title": "Spring Batch write multiple chunks at once/ aggregate writing of chunks after reaching threshold",
    "body": "<p>I am using Spring Batch for the first time.</p>\n<p>Currently, my setup is as follows:</p>\n<ul>\n<li>one Kafka reader, one item processor, one JDBC writer (custom one)</li>\n<li>the Kafka reader reads items from Kafka</li>\n<li>chunk size is 25</li>\n</ul>\n<p>Bascially, 25 items are read from the Kafka Topic for 1 chunk.\nNow, 1 Kafka item contains 1...n child items. This child item size is not fixed, it differs for every read Kafka item.\nEvery Kafka item is flattened in my processor, so that the processor returns a list of the child items.\nThe writer then gets the list of child items of the full chunk.\nSo far so good.</p>\n<p>What this means is, that every time my writer does a write to the database, the count of items that are actually written can be different. One time it might write 150 items, the next chunk it might be only 50 items and so on.</p>\n<p>Is there a way to create a writer that only writes to the database when a specific child item threshold is reached?\nFor example, I want the writer to write the child items if a minimal count of 1000 items is reached. That way I want to improve write performance when writing to my Postgres database.\nBascially, I want to aggregate the write operation for multiple chunks while still maintaining the commit behaviour of Spring Batch.</p>\n<p>I had the idea of writing a custom writer that is using some internal buffer. But I can't get my head around the commit/transaction behaviour of Spring Batch. How shall I tell Spring Batch that it only commits the written items when the writer actually writes data after reaching the threshold? Maybe it's just not possible.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}