{
  "question": {
    "tags": [
      "java",
      "google-cloud-dataflow",
      "apache-beam"
    ],
    "owner": {
      "account_id": 12543328,
      "reputation": 17,
      "user_id": 13346478,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/leTEn.jpg?s=256",
      "display_name": "James Koh",
      "link": "https://stackoverflow.com/users/13346478/james-koh"
    },
    "is_answered": false,
    "view_count": 106,
    "answer_count": 1,
    "score": -1,
    "last_activity_date": 1724912366,
    "creation_date": 1724400345,
    "question_id": 78904895,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/78904895/google-dataflow-limit-total-worker-count-across-all-jobs",
    "title": "Google Dataflow limit total worker count across all jobs",
    "body": "<p>We are using Google Dataflow (Apache Beam + Java) to run batch data jobs (that connect to BigQuery). In order to limit cost, we are using BigQuery reservations (as an example, reserve 3000 (just some arbitrary number) slots).</p>\n<p>The issue is that when there are multiple jobs running at the same time, we don't have control over how many workers will be spawned in total across all jobs. This could cause BigQuery to throw errors like <code>Exceeded rate limits: too many api requests per user per method for this user_method (JobService.query)</code>.</p>\n<p>Is there a way to set how many workers in total Google Dataflow can use across all jobs?</p>\n<p>Based on my research, we could use the following way:</p>\n<ol>\n<li>Dataflow limits concurrent running jobs to 25, and we could request to change this value.</li>\n<li>Use <code>--maxNumWorkers=120</code> flag when starting up a job\nThus 25 * 120 = 3000 workers, and the limit won't be exceeded. But this is not ideal as some jobs might not fully utilize all 120 workers.</li>\n</ol>\n<p>Thus, I would like to find a more direct solution.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}