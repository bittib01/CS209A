{
  "question": {
    "tags": [
      "python",
      "java",
      "artificial-intelligence"
    ],
    "owner": {
      "account_id": 26747337,
      "reputation": 89,
      "user_id": 20347223,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/aaf01f1b26bbabec2c257ce75b2ed996?s=256&d=identicon&r=PG",
      "display_name": "Ch Vamsi",
      "link": "https://stackoverflow.com/users/20347223/ch-vamsi"
    },
    "is_answered": true,
    "view_count": 174,
    "answer_count": 4,
    "score": 1,
    "last_activity_date": 1763476027,
    "creation_date": 1763356009,
    "question_id": 79821921,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79821921/data-extraction-from-pdf",
    "title": "Data extraction from PDF",
    "body": "<p>Dealing with large volumes of PDFs has been challenging lately. Our primary focus is accurately extracting data from PDFs and in some cases the volume can go up to 100,000 files in a single batch. Tesseract-based OCR is too slow for this use case, especially with mixed file sizes, image-heavy pages, and the need to extract data from specific regions of each document. On top of that, I often have to identify which pages need to be rearranged based on client instructions. I’m exploring faster alternatives and tools for high-volume OCR, page identification/reordering, and data extraction.</p>\n<p>If you’ve solved similar problems or have suggestions for better ways to handle this, I’d really appreciate your insights in the comments.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}