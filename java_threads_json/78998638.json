{
  "question": {
    "tags": [
      "java",
      "json",
      "apache-kafka",
      "apache-kafka-connect",
      "boomi"
    ],
    "owner": {
      "account_id": 35726085,
      "reputation": 1,
      "user_id": 27367802,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/72ecd0c8d48786370fb2f3514a29efb6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hemalathaa",
      "link": "https://stackoverflow.com/users/27367802/hemalathaa"
    },
    "is_answered": false,
    "view_count": 71,
    "answer_count": 1,
    "score": -1,
    "last_activity_date": 1727271344,
    "creation_date": 1726668259,
    "last_edit_date": 1726751227,
    "question_id": 78998638,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/78998638/unable-to-sent-records-to-db-from-kafka-topic-using-microsoft-sql-server-sink-co",
    "title": "Unable to sent records to Db from kafka topic using Microsoft sql server sink connector",
    "body": "<p><strong>Summary:</strong> I need to send records to kafka topic and using microsoft sql server sink connector have to fetch the record from topic and send to database.\n**\nApproaches i tried for sending records to topic.**</p>\n<p>1)Manual Approach (Produce a message button)</p>\n<p>2)Rest api (postman)</p>\n<p>3)Java producer application (using Ide)</p>\n<p>4)Boomi process</p>\n<p>**</p>\n<p>Concern:**</p>\n<ul>\n<li><p>Whenever i produce records manually (in confluent cloud produce message button) it sent to topic and mssql sink connector will processed the record and sent to db -â†’ This case is succeed</p>\n</li>\n<li><p>But if i send with the help of 3rd parties like Boomi, Postman, Java application ==&gt; Records sent to topic but the mssql sink connector will push the records into DLQ, it wont sent to Db.\nThis is the error i noticed in DLQ message.\n<a href=\"https://i.sstatic.net/zOkJ4Wd5.png\" rel=\"nofollow noreferrer\">This is the schema i added for the topic</a></p>\n</li>\n</ul>\n<pre><code>[   {     &quot;key&quot;: &quot;__connect.errors.topic&quot;,     &quot;value&quot;: &quot;sample_data_test&quot;   },   {     &quot;key&quot;: &quot;__connect.errors.partition&quot;,     &quot;value&quot;: &quot;5&quot;   },   {     &quot;key&quot;: &quot;__connect.errors.offset&quot;,     &quot;value&quot;: &quot;12&quot;   },   {     &quot;key&quot;: &quot;__connect.errors.connector.name&quot;,     &quot;value&quot;: &quot;lcc-vk28vj&quot;   },   {     &quot;key&quot;: &quot;__connect.errors.task.id&quot;,     &quot;value&quot;: &quot;0&quot;   },   {     &quot;key&quot;: &quot;__connect.errors.stage&quot;,     &quot;value&quot;: &quot;VALUE_CONVERTER&quot;   },   {     &quot;key&quot;: &quot;__connect.errors.class.name&quot;,     &quot;value&quot;: &quot;io.confluent.connect.json.JsonSchemaConverter&quot;   },   {     &quot;key&quot;: &quot;__connect.errors.exception.class.name&quot;,     &quot;value&quot;: &quot;org.apache.kafka.connect.errors.DataException&quot;   },   {     &quot;key&quot;: &quot;__connect.errors.exception.message&quot;,     &quot;value&quot;: &quot;Converting byte[] to Kafka Connect data failed due to serialization error of topic sample_data_test: &quot;   },   {     &quot;key&quot;: &quot;__connect.errors.exception.stacktrace&quot;,     &quot;value&quot;: &quot;org.apache.kafka.connect.errors.DataException: **Converting byte[] to Kafka Connect data failed due to serialization error of topic sample_data_test:** \\n\\tat io.confluent.connect.json.JsonSchemaConverter.toConnectData(JsonSchemaConverter.java:144)\\n\\tat org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$5(WorkerSinkTask.java:546)\\n\\tat org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:217)\\n\\tat org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:254)\\n\\tat org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:189)\\n\\tat org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:546)\\n\\tat org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:521)\\n\\tat org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:347)\\n\\tat org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)\\n\\tat org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)\\n\\tat org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:247)\\n\\tat org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:302)\\n\\tat org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$7(Plugins.java:339)\\n\\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\\n\\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\nCaused by: org.apache.kafka.common.errors.SerializationException: Error deserializing JSON message for id -1\\n\\tat io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaDeserializer.deserialize(AbstractKafkaJsonSchemaDeserializer.java:238)\\n\\tat io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaDeserializer.deserializeWithSchemaAndVersion(AbstractKafkaJsonSchemaDeserializer.java:315)\\n\\tat io.confluent.connect.json.JsonSchemaConverter$Deserializer.deserialize(JsonSchemaConverter.java:193)\\n\\tat io.confluent.connect.json.JsonSchemaConverter.toConnectData(JsonSchemaConverter.java:127)\\n\\t... 17 more\\nCaused by: org.apache.kafka.common.errors.SerializationException: Unknown magic byte!\\n\\tat io.confluent.kafka.serializers.AbstractKafkaSchemaSerDe.getByteBuffer(AbstractKafkaSchemaSerDe.java:638)\\n\\tat io.confluent.kafka.serializers.json.AbstractKafkaJsonSchemaDeserializer.deserialize(AbstractKafkaJsonSchemaDeserializer.java:129)\\n\\t... 20 more\\n&quot;   } ]\n</code></pre>\n<ul>\n<li>So, In Java producer application what i have done is before sending message to topic i did the serialization also.</li>\n</ul>\n<p>**1)Is there any Limitation for mssql sink connector (while sending records through 3rd parties)?</p>\n<p>2)Is there any other approach?**</p>\n<p>(Whenever i send records to kafka topic via boomi process using kafka connector, restapi, or java producer application)Mssql sink connector should process the record and send to db.\nNote: By using the above approaches data sent to topic but mssql sink can't processed these records.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}