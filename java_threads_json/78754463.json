{
  "question": {
    "tags": [
      "java",
      "apache-spark",
      "hadoop",
      "delta-lake",
      "minio"
    ],
    "owner": {
      "account_id": 25038061,
      "reputation": 49,
      "user_id": 18895248,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6382f5c6ac2f7dec7d60fbb19744191e?s=256&d=identicon&r=PG",
      "display_name": "juliastoelli",
      "link": "https://stackoverflow.com/users/18895248/juliastoelli"
    },
    "is_answered": true,
    "view_count": 500,
    "accepted_answer_id": 78792052,
    "answer_count": 2,
    "score": -2,
    "last_activity_date": 1721893726,
    "creation_date": 1721131614,
    "last_edit_date": 1721132152,
    "question_id": 78754463,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/78754463/getting-java-lang-unsatisfiedlinkerror-when-trying-to-run-my-code",
    "title": "Getting java.lang.UnsatisfiedLinkError when trying to run my Code",
    "body": "<p>i am trying to follw this tutorial <a href=\"https://dzone.com/articles/databricks-delta-lake-using-java\" rel=\"nofollow noreferrer\">https://dzone.com/articles/databricks-delta-lake-using-java</a>\nSo i want to use spark with delta lake, but for my storage i use minio and not hdfs or s3 (But it is incorporated the same way).\nMaybe someone can help me with my problem, so here is my code:</p>\n<pre><code>package minIO;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.spark.sql.DataFrameWriter;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Encoder;\nimport org.apache.spark.sql.Encoders;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n\n\npublic class MinIOConnector {\n    public static void main(String[] args) throws IOException {\n\n     SparkSession spark = SparkSession.builder()\n                .appName(&quot;Spark Excel file conversion&quot;)\n                .config(&quot;spark.master&quot;, &quot;local&quot;)\n                .config(&quot;spark.sql.extensions&quot;, &quot;io.delta.sql.DeltaSparkSessionExtension&quot;)\n                .config(&quot;spark.sql.catalog.spark_catalog&quot;, &quot;org.apache.spark.sql.delta.catalog.DeltaCatalog&quot;)\n                .config(&quot;spark.hadoop.fs.s3a.access.key&quot;, &quot;&quot;)\n                .config(&quot;spark.hadoop.fs.s3a.secret.key&quot;, &quot;&quot;)\n                .config(&quot;spark.hadoop.fs.s3a.endpoint&quot;, &quot;&quot;)\n                .config(&quot;spark.databricks.delta.retentionDurationCheck.enabled&quot;, &quot;false&quot;)\n                .config(&quot;spark.hadoop.fs.s3a.aws.credentials.provider&quot;, &quot;org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider&quot;)\n                .config(&quot;spark.sql.warehouse.dir&quot;, &quot;file://./sparkapp&quot;)\n                .config(&quot;spark.hadoop.io.native.lib.available&quot;, &quot;false&quot;)\n                .config(&quot;spark.hadoop.fs.file.impl&quot;, &quot;org.apache.hadoop.fs.LocalFileSystem&quot;)\n                .getOrCreate();\n\n\n\n    List&lt;Employee&gt; empList = new ArrayList&lt;Employee&gt;();\n\n            Employee emp = new Employee();\n\n            emp.setEmpId(&quot;1234&quot;);\n\n            emp.setEmpName(&quot;kiran&quot;);\n\n            emp.setDeptName(&quot;Design dept&quot;);\n\n            empList.add(emp);\n\n            emp = new Employee();\n\n            emp.setEmpId(&quot;3567&quot;);\n\n            emp.setEmpName(&quot;raju&quot;);\n\n            emp.setDeptName(&quot;IT&quot;);\n\n            empList.add(emp);\n\nEncoder&lt;Employee&gt; employeeEncoder = Encoders.bean(Employee.class);\n                Dataset&lt;org.apache.spark.sql.Row&gt; empDF = spark.createDataset(empList, employeeEncoder).toDF();\n                DataFrameWriter&lt;Row&gt; x = empDF.write().format(&quot;delta&quot;);\n\n                x.save(&quot;s3a://test/test&quot;);\n                System.out.println(&quot;after write&quot;);\n\n                empDF = spark.read().format(&quot;delta&quot;).load(&quot;s3a://test/test&quot;);\n                empDF.show();\n    }\n\n}\n</code></pre>\n<p>this is my pom.xml:</p>\n<pre><code>&lt;project xmlns=&quot;https://maven.apache.org/POM/4.0.0&quot;\nxmlns:xsi=&quot;https://www.w3.org/2001/XMLSchema-instance&quot;\nxsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;\n&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n&lt;groupId&gt;minio&lt;/groupId&gt;\n&lt;artifactId&gt;minio&lt;/artifactId&gt;\n&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\n&lt;properties&gt;\n    &lt;!-- Your exact Hadoop version here--&gt;\n    &lt;hadoop.version&gt;3.0.0&lt;/hadoop.version&gt;\n&lt;/properties&gt;\n\n&lt;dependencies&gt;      \n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n        &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;\n        &lt;version&gt;3.5.1&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;\n        &lt;artifactId&gt;spark-sql_2.12&lt;/artifactId&gt;\n        &lt;version&gt;3.5.1&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;io.delta&lt;/groupId&gt;\n            &lt;artifactId&gt;delta-spark_2.12&lt;/artifactId&gt;\n            &lt;version&gt;3.2.0&lt;/version&gt;\n        &lt;/dependency&gt;\n\n\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;\n            &lt;version&gt;${hadoop.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n            &lt;artifactId&gt;hadoop-aws&lt;/artifactId&gt;\n            &lt;version&gt;${hadoop.version}&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;\n    &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;\n    &lt;version&gt;${hadoop.version}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;\n    &lt;artifactId&gt;aws-java-sdk-bundle&lt;/artifactId&gt;\n    &lt;version&gt;1.11.199&lt;/version&gt;\n&lt;/dependency&gt;\n        &lt;dependency&gt;\n    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;\n    &lt;artifactId&gt;jackson-core&lt;/artifactId&gt;\n    &lt;version&gt;2.17.2&lt;/version&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n    &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\n    &lt;version&gt;2.0.13&lt;/version&gt;\n&lt;/dependency&gt;\n    &lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre>\n<p>my installed spark version is 3.5.1 from here <a href=\"https://spark.apache.org/downloads.html\" rel=\"nofollow noreferrer\">https://spark.apache.org/downloads.html</a>\nand hadoop 3.0.0 from here <a href=\"https://github.com/steveloughran/winutils/tree/master\" rel=\"nofollow noreferrer\">https://github.com/steveloughran/winutils/tree/master</a></p>\n<p>this is my stack trace:</p>\n<blockquote>\n<p>Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most\nrecent failure: Lost task 0.0 in stage 0.0 (TID 0)\n(BSDESK295.bsbanksysteme.com executor driver):\njava.lang.UnsatisfiedLinkError:\norg.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\nat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native\nMethod)   at\norg.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\nat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)    at\norg.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:160)\nat\norg.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:100)\nat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:77)\nat\norg.apache.hadoop.util.BasicDiskValidator.checkStatus(BasicDiskValidator.java:32)\nat\norg.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:330)\nat\norg.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)\nat\norg.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:476)\nat\norg.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:213)\nat\norg.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:479)\nat\norg.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:811)\nat\norg.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:185)\nat\norg.apache.hadoop.fs.s3a.S3ABlockOutputStream.(S3ABlockOutputStream.java:167)\nat\norg.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:657)\nat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)     at\norg.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)  at\norg.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\nat\norg.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:347)\nat\norg.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:314)\nat\norg.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\nat\norg.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\nat\norg.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\nat\norg.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:36)\nat\norg.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\nat\norg.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\nat\norg.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.(FileFormatDataWriter.scala:146)\nat\norg.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:412)\nat\norg.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$2(DeltaFileFormatWriter.scala:274)\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\nat\norg.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\nat org.apache.spark.scheduler.Task.run(Task.scala:141)  at\norg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\nat\norg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\nat\norg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\nat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\nat\norg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\nat\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:750)</p>\n<p>Driver stacktrace:    at\norg.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\nat\norg.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\nat\norg.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\nat scala.collection.immutable.List.foreach(List.scala:333)  at\norg.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\nat\norg.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\nat\norg.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\nat scala.Option.foreach(Option.scala:437)   at\norg.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\nat\norg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\nat\norg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\nat\norg.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\nat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nat\norg.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\nat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)    at\norg.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$1(DeltaFileFormatWriter.scala:263)\nat\norg.apache.spark.sql.delta.files.DeltaFileFormatWriter$.writeAndCommit(DeltaFileFormatWriter.scala:295)\nat\norg.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeWrite(DeltaFileFormatWriter.scala:234)\nat\norg.apache.spark.sql.delta.files.DeltaFileFormatWriter$.write(DeltaFileFormatWriter.scala:214)\nat\norg.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:440)\nat\norg.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\nat\norg.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\nat\norg.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\nat\norg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\nat\norg.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\nat\norg.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:398)\nat\norg.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:371)\nat\norg.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\nat\norg.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:246)\nat\norg.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:242)\nat\norg.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\nat\norg.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:235)\nat\norg.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:232)\nat\norg.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\nat\norg.apache.spark.sql.delta.commands.WriteIntoDelta.writeFiles(WriteIntoDelta.scala:349)\nat\norg.apache.spark.sql.delta.commands.WriteIntoDelta.writeAndReturnCommitData(WriteIntoDelta.scala:312)\nat\norg.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:106)\nat\norg.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:101)\nat\norg.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:227)\nat\norg.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:101)\nat\norg.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:201)\nat\norg.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\nat\norg.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\nat\norg.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\nat\norg.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\nat\norg.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\nat\norg.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\nat\norg.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\nat\norg.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\nat\norg.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\nat\norg.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\nat\norg.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\nat\norg.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\nat\norg.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\nat\norg.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\nat\norg.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\nat\norg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\nat\norg.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\nat\norg.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\nat\norg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\nat\norg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\nat\norg.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\nat\norg.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\nat\norg.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\nat\norg.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\nat\norg.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\nat\norg.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\nat\norg.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\nat\norg.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\nat\norg.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\nat minIO.MinIOConnector.main(MinIOConnector.java:78) Caused by:\njava.lang.UnsatisfiedLinkError:\norg.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\nat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native\nMethod)   at\norg.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\nat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)    at\norg.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:160)\nat\norg.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:100)\nat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:77)\nat\norg.apache.hadoop.util.BasicDiskValidator.checkStatus(BasicDiskValidator.java:32)\nat\norg.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:330)\nat\norg.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:393)\nat\norg.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:476)\nat\norg.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:213)\nat\norg.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:479)\nat\norg.apache.hadoop.fs.s3a.S3ADataBlocks$DiskBlockFactory.create(S3ADataBlocks.java:811)\nat\norg.apache.hadoop.fs.s3a.S3ABlockOutputStream.createBlockIfNeeded(S3ABlockOutputStream.java:185)\nat\norg.apache.hadoop.fs.s3a.S3ABlockOutputStream.(S3ABlockOutputStream.java:167)\nat\norg.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:657)\nat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)     at\norg.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)  at\norg.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\nat\norg.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:347)\nat\norg.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:314)\nat\norg.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\nat\norg.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\nat\norg.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\nat\norg.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:36)\nat\norg.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\nat\norg.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\nat\norg.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.(FileFormatDataWriter.scala:146)\nat\norg.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeTask(DeltaFileFormatWriter.scala:412)\nat\norg.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$2(DeltaFileFormatWriter.scala:274)\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\nat\norg.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\nat org.apache.spark.scheduler.Task.run(Task.scala:141)  at\norg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\nat\norg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\nat\norg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\nat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\nat\norg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\nat\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\nat\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\nat java.lang.Thread.run(Thread.java:750)</p>\n</blockquote>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}