{
  "question": {
    "tags": [
      "java",
      "apache-kafka",
      "apache-kafka-streams",
      "confluent-schema-registry"
    ],
    "owner": {
      "account_id": 14846137,
      "reputation": 1,
      "user_id": 10721407,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/13d0e9182604d2794f8670257e70731f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Carl",
      "link": "https://stackoverflow.com/users/10721407/carl"
    },
    "is_answered": false,
    "view_count": 164,
    "answer_count": 1,
    "score": 0,
    "last_activity_date": 1735551241,
    "creation_date": 1721038139,
    "last_edit_date": 1721038224,
    "question_id": 78749266,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/78749266/my-kafka-streams-is-failing-to-connect-to-the-upstash-schema-registry",
    "title": "My kafka streams is failing to connect to the upstash schema registry",
    "body": "<p>Curling this <a href=\"https://right-boa-11231-eu1-rest-kafka.upstash.io/schema-registry/schemas/ids/8?fetchMaxId=false&amp;subject=test1-value\" rel=\"nofollow noreferrer\">https://right-boa-11231-eu1-rest-kafka.upstash.io/schema-registry/schemas/ids/8?fetchMaxId=false&amp;subject=test1-value</a> returns</p>\n<pre><code>{&quot;schema&quot;: &quot;{\\&quot;type\\&quot;:\\&quot;record\\&quot;,\\&quot;name\\&quot;:\\&quot;User\\&quot;,\\&quot;namespace\\&quot;:\\&quot;fr.potato\\&quot;,\\&quot;fields\\&quot;:[{\\&quot;name\\&quot;:\\&quot;name\\&quot;,\\&quot;type\\&quot;:\\&quot;string\\&quot;},{\\&quot;name\\&quot;:\\&quot;favorite_number\\&quot;,\\&quot;type\\&quot;:\\&quot;long\\&quot;},{\\&quot;name\\&quot;:\\&quot;timestamp_string\\&quot;,\\&quot;type\\&quot;:\\&quot;string\\&quot;}]}&quot;}\n\n</code></pre>\n<p>hence it seems like the problem is with my Kstreams app and not the schema registry. I have tried every configuration under the sun, but I am still getting this exception.</p>\n<pre><code>2024-07-15 12:50:00 DEBUG StreamThread:1201 - stream-thread [enrichement-app-14-7e65669d-662f-44a6-b47a-30af74085b4e-StreamThread-1] Main Consumer poll completed in 133 ms and fetched 1 records from partitions [test1-0]\n2024-07-15 12:50:00 DEBUG RestService:292 - Sending GET with input null to https://right-boa-11231-eu1-rest-kafka.upstash.io/schema-registry/schemas/ids/8?fetchMaxId=false&amp;subject=test1-value\n2024-07-15 12:50:00 ERROR LogAndFailExceptionHandler:39 - \nException\n caught during Deserialization, taskId: 0_0, topic: test1, partition: 0, offset: 0\norg.apache.kafka.common.errors.SerializationException\n: Error retrieving Avro value schema for id 8\n  at io.confluent.kafka.serializers.AbstractKafkaSchemaSerDe.toKafkaException(AbstractKafkaSchemaSerDe.java:805) ~[kafka-schema-serializer-7.6.1.jar:?]\n  at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer$DeserializationContext.schemaFromRegistry(AbstractKafkaAvroDeserializer.java:415) ~[kafka-avro-serializer-7.6.1.jar:?]\n  at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer.deserialize(AbstractKafkaAvroDeserializer.java:188) ~[kafka-avro-serializer-7.6.1.jar:?]\n  at io.confluent.kafka.serializers.KafkaAvroDeserializer.deserialize(KafkaAvroDeserializer.java:107) ~[kafka-avro-serializer-7.6.1.jar:?]\n  at io.confluent.kafka.serializers.KafkaAvroDeserializer.deserialize(KafkaAvroDeserializer.java:102) ~[kafka-avro-serializer-7.6.1.jar:?]\n  at io.confluent.kafka.streams.serdes.avro.GenericAvroDeserializer.deserialize(GenericAvroDeserializer.java:63) ~[kafka-streams-avro-serde-5.2.1.jar:?]\n  at io.confluent.kafka.streams.serdes.avro.GenericAvroDeserializer.deserialize(GenericAvroDeserializer.java:39) ~[kafka-streams-avro-serde-5.2.1.jar:?]\n  at org.apache.kafka.common.serialization.Deserializer.deserialize(Deserializer.java:62) ~[kafka-clients-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.SourceNode.deserializeValue(SourceNode.java:58) ~[kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.RecordDeserializer.deserialize(RecordDeserializer.java:66) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.RecordQueue.updateHead(RecordQueue.java:204) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.RecordQueue.addRawRecords(RecordQueue.java:128) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.PartitionGroup.addRawRecords(PartitionGroup.java:285) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.StreamTask.addRecords(StreamTask.java:1039) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.TaskManager.addRecordsToTasks(TaskManager.java:1782) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.StreamThread.pollPhase(StreamThread.java:1208) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.StreamThread.runOnceWithoutProcessingThreads(StreamThread.java:909) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:686) [kafka-streams-3.7.1.jar:?]\n  at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:645) [kafka-streams-3.7.1.jar:?]\nCaused by: \nio.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException\n: null; error code: 0\n  at io.confluent.kafka.schemaregistry.client.rest.RestService.sendHttpRequest(RestService.java:336) ~[kafka-schema-registry-client-7.6.1.jar:?]\n  at io.confluent.kafka.schemaregistry.client.rest.RestService.httpRequest(RestService.java:409) ~[kafka-schema-registry-client-7.6.1.jar:?]\n  at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:916) ~[kafka-schema-registry-client-7.6.1.jar:?]\n  at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:900) ~[kafka-schema-registry-client-7.6.1.jar:?]\n  at io.confluent.kafka.schemaregistry.client.rest.RestService.getId(RestService.java:880) ~[kafka-schema-registry-client-7.6.1.jar:?]\n  at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getSchemaByIdFromRegistry(CachedSchemaRegistryClient.java:333) ~[kafka-schema-registry-client-7.6.1.jar:?]\n  at io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient.getSchemaBySubjectAndId(CachedSchemaRegistryClient.java:464) ~[kafka-schema-registry-client-7.6.1.jar:?]\n  at io.confluent.kafka.serializers.AbstractKafkaAvroDeserializer$DeserializationContext.schemaFromRegistry(AbstractKafkaAvroDeserializer.java:398) ~[kafka-avro-serializer-7.6.1.jar:?]\n  ... 17 more\n</code></pre>\n<p>this is my app</p>\n<pre><code>package fr.potato;\n\nimport java.util.Collections;\nimport java.util.Properties;\nimport java.util.Map;\n\nimport org.apache.avro.generic.GenericRecord;\nimport org.apache.kafka.common.serialization.Serde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.errors.LogAndContinueExceptionHandler;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.KStream;\nimport org.apache.kafka.streams.kstream.Produced;\nimport io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\nimport io.confluent.kafka.streams.serdes.avro.GenericAvroSerde;\nimport io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\n\npublic class Enrichement {\n  static String auth = &quot;username:pw&quot;;\n  static String sourceTopicName = &quot;test1&quot;;\n  static String targetTopicName = &quot;test2&quot;;\n  static String schemaRegistryUrl = &quot;https://right-boa-11231-eu1-rest-kafka.upstash.io/schema-registry&quot;;\n  private static final Logger logger = LogManager.getLogger(Enrichement.class);\n\n  // @SuppressWarnings(&quot;deprecation&quot;)\n  public static void main(String[] args) {\n    Properties props = new Properties();\n    props.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;enrichement-app-14&quot;);\n    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;https://right-boa-11231-eu1-kafka.upstash.io:9092&quot;);\n    props.put(&quot;sasl.mechanism&quot;, &quot;SCRAM-SHA-256&quot;);\n    props.put(&quot;security.protocol&quot;, &quot;SASL_SSL&quot;);\n    props.put(&quot;sasl.jaas.config&quot;,\n        &quot;org.apache.kafka.common.security.scram.ScramLoginModule required username=\\&quot;username\\&quot; password=\\&quot;pw\\&quot;;&quot;);\n    props.put(&quot;basic.auth.credentials.source&quot;, &quot;USER_INFO&quot;);\n    props.put(&quot;basic.auth.user.info&quot;, auth);\n    // https://right-boa-11231-eu1-rest-kafka.upstash.io/schema-registry/schemas/ids/8?fetchMaxId=false&amp;subject=test1-value\n    props.put(&quot;schema.registry.url&quot;, &quot;right-boa-11231-eu1-rest-kafka.upstash.io/schema-registry&quot;);\n    props.put(&quot;debug&quot;, &quot;true&quot;);\n    props.put(&quot;ssl.endpoint.identification.algorithm&quot;, &quot;https&quot;);\n\n    props.put(&quot;key.converter&quot;, &quot;org.apache.kafka.connect.storage.StringConverter&quot;);\n    props.put(&quot;value.converter&quot;, &quot;io.confluent.connect.avro.AvroConverter&quot;);\n    props.put(&quot;value.converter.schema.registry.url&quot;, &quot;right-boa-11231-eu1-rest-kafka.upstash.io/schema-registry&quot;);\n\n    props.put(&quot;key.converter.basic.auth.credentials.source&quot;, &quot;USER_INFO&quot;);\n    props.put(&quot;key.converter.basic.auth.user.info&quot;, auth);\n    props.put(&quot;value.converter.basic.auth.credentials.source&quot;, &quot;USER_INFO&quot;);\n    props.put(&quot;value.converter.basic.auth.user.info&quot;, auth);\n\n    props.put(&quot;auto.register.schemas&quot;, false);\n    props.put(&quot;use.latest.version&quot;, true);\n\n    final Map&lt;String, String&gt; serdeConfig = Collections.singletonMap(\n        &quot;schema.registry.url&quot;, schemaRegistryUrl);\n\n    final Serde&lt;GenericRecord&gt; valueGenericAvroSerde = new GenericAvroSerde();\n    valueGenericAvroSerde.configure(serdeConfig, false); // `false` for record values\n\n    StreamsBuilder builder = new StreamsBuilder();\n    KStream&lt;String, GenericRecord&gt; inputStream = builder.stream(sourceTopicName,\n        Consumed.with(Serdes.String(), valueGenericAvroSerde));\n\n    inputStream\n        .peek((key, value) -&gt; System.out.println(&quot;Key: &quot; + key + &quot;, Value: &quot; + value))\n        .to(targetTopicName, Produced.with(Serdes.String(), valueGenericAvroSerde));\n\n    try {\n      KafkaStreams streams = new KafkaStreams(builder.build(), props);\n      streams.setUncaughtExceptionHandler((Thread thread, Throwable throwable) -&gt; {\n        logger.error(&quot;Uncaught exception in thread &quot; + thread, throwable);\n      });\n      streams.start();\n      System.out.println(&quot;Kafka Streams app started successfully.&quot;);\n\n      Runtime.getRuntime().addShutdownHook(new Thread(streams::close));\n    } catch (Exception e) {\n      System.err.println(&quot;Error starting Kafka Streams app: &quot; + e.getMessage());\n      e.printStackTrace();\n    }\n  }\n}\n</code></pre>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}