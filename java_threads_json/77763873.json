{
  "question": {
    "tags": [
      "java",
      "docker",
      "out-of-memory",
      "heap-memory"
    ],
    "owner": {
      "account_id": 4646120,
      "reputation": 1103,
      "user_id": 3764361,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://www.gravatar.com/avatar/3adb0afe3b4e5350ea48cdcc99f3337d?s=256&d=identicon&r=PG",
      "display_name": "Phillip",
      "link": "https://stackoverflow.com/users/3764361/phillip"
    },
    "is_answered": true,
    "view_count": 1475,
    "closed_date": 1730902320,
    "answer_count": 5,
    "score": 3,
    "last_activity_date": 1731050042,
    "creation_date": 1704449905,
    "last_edit_date": 1704451087,
    "question_id": 77763873,
    "link": "https://stackoverflow.com/questions/77763873/docker-kills-java-container-due-to-oom-but-no-signs-of-high-memory-usage-in-jmx",
    "closed_reason": "Not suitable for this site",
    "title": "Docker kills java container due to OOM but no signs of high memory usage in JMX statistics",
    "body": "<p>We have a Docker container with a Java 11 application running on AWS ECS. The service is configured to be killed once it uses 1,5GB of RAM</p>\n<pre><code>&quot;memory&quot;: 1500,\n&quot;memoryReservation&quot;: 1137,\n</code></pre>\n<p><a href=\"https://i.sstatic.net/SLfPq.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/SLfPq.png\" alt=\"AWS ECS metrics of said service\" /></a></p>\n<p>The problem is that we don't see this increased memory usage reflected in our dashboards\n<a href=\"https://i.sstatic.net/ie7OD.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/ie7OD.png\" alt=\"enter image description here\" /></a>\n<a href=\"https://i.sstatic.net/LXGyd.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/LXGyd.png\" alt=\"enter image description here\" /></a>\n<a href=\"https://i.sstatic.net/77j39.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/77j39.png\" alt=\"enter image description here\" /></a>\n(p.s. the flatline in the grafana dashboards is due to Prometheus no longer receiving metrics).</p>\n<p>We have done pretty much everything possible to limit the amount of on- and offheap memory Java is allowed to use and if I check the output of NMT it's reserved usage is at most 1,1GB. Our command line right now is this:</p>\n<pre><code>java ‑Xmx600m ‑XX:+UseG1GC ‑XX:MaxMetaspaceSize=220m \n‑XX:+PrintFlagsFinal ‑XX:NewRatio=1 ‑XX:MaxDirectMemorySize=40m \n‑Xss512k ‑XX:ProfiledCodeHeapSize=70M ‑XX:NonProfiledCodeHeapSize=50M \n‑XX:NonNMethodCodeHeapSize=8M ‑XX:+UseCodeCacheFlushing \n‑XX:CompressedClassSpaceSize=32M ‑XX:ReservedCodeCacheSize=128M \n‑XX:+SegmentedCodeCache ‑javaagent:/opentelemetry‑javaagent.jar \n‑Dotel.javaagent.extensions=/opentelemetry‑java‑ecs‑extension‑all.jar \n‑Dotel.resource.attributes=xxx,aws.ecs.launchtype=ec2 \n‑XX:NativeMemoryTracking=summary ‑XX:‑OmitStackTraceInFastThrow \n‑Dspring.profiles.active=prd‑ecs ‑Dspring.cloud.consul.host=172.17.42.1 \n‑Dspring.cloud.consul.config.watch.enabled=false \n‑Dcom.sun.management.jmxremote.port=9999 \n‑Dcom.sun.management.jmxremote.authenticate=false \n‑Dcom.sun.management.jmxremote.ssl=false \n‑Dlog4j2.formatMsgNoLookups=true ‑Dserver.port=8080 \n‑Daws.paramstore.enabled=true \n‑Daws.paramstore.region=eu‑west‑1 \n‑Dspring.main.banner‑mode=off ‑jar /1234‑service.jar \n</code></pre>\n<p>We are kind of clueless on how it's possible that all of a sudden our containers use 400MB memory without any sign of it in the JMX metrics.</p>\n<p>//edit: the output of the NMT of the oldest running container</p>\n<pre><code>Native Memory Tracking:\n\nTotal: reserved=1180115KB, committed=700171KB\n-                 Java Heap (reserved=614400KB, committed=260096KB)\n                            (mmap: reserved=614400KB, committed=260096KB)\n\n-                     Class (reserved=215344KB, committed=208080KB)\n                            (classes #35567)\n                            (  instance classes #33401, array classes #2166)\n                            (malloc=8496KB #125896)\n                            (mmap: reserved=206848KB, committed=199584KB)\n                            (  Metadata:   )\n                            (    reserved=174080KB, committed=173536KB)\n                            (    used=167363KB)\n                            (    free=6173KB)\n                            (    waste=0KB =0.00%)\n                            (  Class space:)\n                            (    reserved=32768KB, committed=26048KB)\n                            (    used=22342KB)\n                            (    free=3706KB)\n                            (    waste=0KB =0.00%)\n\n-                    Thread (reserved=77858KB, committed=16034KB)\n                            (thread #125)\n                            (stack: reserved=77260KB, committed=15436KB)\n                            (malloc=452KB #752)\n                            (arena=146KB #249)\n\n-                      Code (reserved=139426KB, committed=96022KB)\n                            (malloc=7330KB #29103)\n                            (mmap: reserved=132096KB, committed=88692KB)\n\n-                        GC (reserved=70010KB, committed=56862KB)\n                            (malloc=14362KB #61984)\n                            (mmap: reserved=55648KB, committed=42500KB)\n\n-                  Compiler (reserved=896KB, committed=896KB)\n                            (malloc=764KB #2798)\n                            (arena=133KB #5)\n\n-                  Internal (reserved=4509KB, committed=4509KB)\n                            (malloc=4477KB #3670)\n                            (mmap: reserved=32KB, committed=32KB)\n\n-                     Other (reserved=4359KB, committed=4359KB)\n                            (malloc=4359KB #39)\n\n-                    Symbol (reserved=37775KB, committed=37775KB)\n                            (malloc=34219KB #439593)\n                            (arena=3556KB #1)\n\n-    Native Memory Tracking (reserved=10765KB, committed=10765KB)\n                            (malloc=34KB #437)\n                            (tracking overhead=10731KB)\n\n-               Arena Chunk (reserved=190KB, committed=190KB)\n                            (malloc=190KB)\n\n-                   Logging (reserved=4KB, committed=4KB)\n                            (malloc=4KB #194)\n\n-                 Arguments (reserved=20KB, committed=20KB)\n                            (malloc=20KB #545)\n\n-                    Module (reserved=3692KB, committed=3692KB)\n                            (malloc=3692KB #14082)\n\n-              Synchronizer (reserved=860KB, committed=860KB)\n                            (malloc=860KB #7286)\n\n-                 Safepoint (reserved=8KB, committed=8KB)\n                            (mmap: reserved=8KB, committed=8KB)\n</code></pre>\n<p>//edit2: our Docker image we use on our 90+ Docker service</p>\n<pre><code>FROM --platform=$TARGETPLATFORM amazoncorretto:11\n\nRUN yum install -y shadow-utils wget &amp;&amp; yum clean all\nRUN wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar -O /opentelemetry-javaagent.jar\nADD opentelemetry-java-ecs-extension-*-all.jar /opentelemetry-java-ecs-extension-all.jar\nRUN adduser docker\nRUN if [ -f $JAVA_HOME/conf/security/java.security ]; then sed -i 's/^networkaddress.cache.ttl/#networkaddress.cache.ttl/g' $JAVA_HOME/conf/security/java.security; fi &amp;&amp; \\\n    echo &quot;networkaddress.cache.ttl=30&quot; &gt;&gt; $JAVA_HOME/conf/security/java.security\n\nUSER docker\n</code></pre>\n<p>//edit3: forgot to mention that we've also set this env var</p>\n<pre><code>            {\n                &quot;name&quot;: &quot;MALLOC_ARENA_MAX&quot;,\n                &quot;value&quot;: &quot;4&quot;\n            }\n</code></pre>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}