{
  "question": {
    "tags": [
      "java",
      "apache-spark",
      "spark-structured-streaming",
      "dataproc"
    ],
    "owner": {
      "account_id": 7999012,
      "reputation": 936,
      "user_id": 6034446,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/eee81eb4f64c8211d5a40f2a8ff968ca?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "firsni",
      "link": "https://stackoverflow.com/users/6034446/firsni"
    },
    "is_answered": false,
    "view_count": 85,
    "answer_count": 1,
    "score": 0,
    "last_activity_date": 1762251013,
    "creation_date": 1760898609,
    "last_edit_date": 1762251013,
    "question_id": 79794432,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79794432/dataproc-serverless-slow-consume-kafka-topic",
    "title": "dataproc serverless slow consume kafka topic",
    "body": "<p>I use dataproc serverless with java API to read kafka topic. The topic has only 2 partitions.\nThe topic receives 200 msg/sec.\nAfter reading messages I repartition to 100, transform the data then write to BQ.\nSince I repartition I see two stages one with 2 tasks to read data and a second one with 10 that transforms and writes the data.\nWhen having\nstreaming.max.offsets.per.trigger=500\nstreaming.min.offsets.per.trigger=100\nThe task of reading is varying a lot in time between 7sec and 1min50.\nWhile the task that transforms an dwrites data takes around 10 sec.\nAny idea about why it's taking too much time to read data and how to optimize the code ?</p>\n<pre><code>Dataset&lt;Row&gt; dfr = spark\n                .readStream()\n                .format(&quot;org.apache.spark.sql.kafka010.KafkaSourceProvider&quot;)\n                .option(&quot;kafka.bootstrap.servers&quot;, kafkaServers)\n                .option(&quot;kafka.sasl.kerberos.service.name&quot;, &quot;kafka&quot;)\n                .option(&quot;kafka.sasl.mechanism&quot;, &quot;GSSAPI&quot;)\n                .option(&quot;kafka.security.protocol&quot;, &quot;SASL_SSL&quot;)\n                .option(&quot;kafka.ssl.truststore.location&quot;, trustStoreName)\n                .option(&quot;kafka.ssl.truststore.password&quot;, truststorePassword)\n                .option(&quot;kafka.ssl.truststore.type&quot;, &quot;JKS&quot;)\n                .option(&quot;startingOffsets&quot;, &quot;latest&quot;)\n                .option(&quot;kafka.max.partition.fetch.bytes&quot;, &quot;209715200&quot;)  // 200MB per partition\n                .option(&quot;kafka.fetch.max.bytes&quot;, &quot;1048576000&quot;)            // 1000MB total\n                .option(&quot;subscribe&quot;, kafkaTopic)\n                .option(&quot;maxOffsetsPerTrigger&quot;, maxOffsets)\n                .option(&quot;minOffsetsPerTrigger&quot;, minOffsets)\n                .option(&quot;failOnDataLoss&quot;, &quot;false&quot;)\n                .option(&quot;kafka.request.timeout.ms&quot;, 300000)\n                .option(&quot;kafka.session.timeout.ms&quot;, 60000)\n                .load();\n\n        Dataset&lt;Row&gt; dfr2 = dfr.selectExpr(\n                &quot;CAST(topic as STRING) as topic&quot;, &quot;CAST(key AS STRING) AS key&quot;,\n                &quot;CAST(value AS STRING) AS xml&quot;,\n                &quot;timestamp&quot;, &quot;partition&quot;, &quot;offset&quot;).repartition(10);\n\n        StructType outSchema = new StructType()\n                .add(&quot;key&quot;, DataTypes.StringType)\n                .add(&quot;topic&quot;, DataTypes.StringType)\n                .add(&quot;partition&quot;, DataTypes.IntegerType)\n                .add(&quot;offset&quot;, DataTypes.LongType)\n                .add(&quot;JSON_COL&quot;, DataTypes.StringType)\n                .add(&quot;DAT_MAJ_DWH&quot;, DataTypes.StringType);\n\n        // Create proper encoder - cast the result to Encoder&lt;Row&gt;\n        Encoder&lt;Row&gt; encoder = Encoders.row(outSchema);\n\n        Dataset&lt;Row&gt; jsonified = dfr2.mapPartitions(\n                (MapPartitionsFunction&lt;Row, Row&gt;) (Iterator&lt;Row&gt; it) -&gt; {\n                    List&lt;Row&gt; out = new ArrayList&lt;&gt;();\n                    DocumentBuilder builder = XML_BUILDER.get();\n                    while (it.hasNext()) {\n                        Row r = it.next();\n                        String topic = r.getString(0);\n                        String key = r.getString(1);\n                        String xml = r.getString(2);\n                        int part = r.getInt(4);\n                        long offset = r.getLong(5);\n                        String json = null;\n                        try {\n                            builder.reset();\n                            // Pass the XML string, not the Document\n                            json = BusMessageXmlJson.toJson(xml);\n                        } catch (Exception ex) {\n                            System.err.println(&quot;XML parse error partition=&quot; + part +\n                                                       &quot; offset=&quot; + offset + &quot; msg=&quot; + ex.getMessage());\n                        }\n                        String ts = r.getTimestamp(3).toInstant().toString();\n                        out.add(RowFactory.create(key, topic, part, offset, json, ts));\n                    }\n                    return out.iterator();\n                },\n                encoder\n        ).withColumn(\n                &quot;DAT_MAJ_DWH&quot;,\n                date_format(to_timestamp(col(&quot;DAT_MAJ_DWH&quot;)), &quot;yyyy-MM-dd'T'HH:mm:ss.SSSSSS&quot;)\n        ).select(&quot;key&quot;,&quot;topic&quot;,&quot;partition&quot;,&quot;offset&quot;,&quot;JSON_COL&quot;,&quot;DAT_MAJ_DWH&quot;);\n\n\n        StreamingQuery query = jsonified\n                .writeStream()\n                .queryName(&quot;spark-sdh-ndc-streaming-query&quot;)\n                .foreachBatch((batchDF, batchId) -&gt; {\n\n                    // Write this batch to BigQuery using batch API\n                    batchDF.select(&quot;JSON_COL&quot;, &quot;DAT_MAJ_DWH&quot;).write()\n                            .format(&quot;bigquery&quot;)\n                            .option(&quot;temporaryGcsBucket&quot;, tempBucket)\n                            .option(&quot;table&quot;, bigQueryTable)\n                            .option(&quot;createDisposition&quot;, &quot;CREATE_IF_NEEDED&quot;)\n                            .option(&quot;intermediateFormat&quot;, &quot;avro&quot;)\n                            .option(&quot;writeMethod&quot;, &quot;indirect&quot;)\n                            .option(&quot;allowFieldAddition&quot;, &quot;true&quot;)\n                            .option(&quot;allowFieldRelaxation&quot;, &quot;true&quot;)\n                            .mode(SaveMode.Append)\n                            .save();\n\n                    // 2. Commit offsets to Kafka AFTER successful write\n                    commitOffsetsToKafka(batchDF, kafkaServers, trustStoreName, truststorePassword, consumerGroup);\n\n                    System.out.println(&quot;Batch &quot; + batchId + &quot; written successfully&quot;);\n                })\n                .option(&quot;checkpointLocation&quot;, checkpointPath)\n                .trigger(Trigger.ProcessingTime(triggerInterval))\n                .start();\n        System.out.println(&quot;Streaming query started successfully!&quot;);\n        System.out.println(&quot;Query ID: &quot; + query.id());\n        System.out.println(&quot;Waiting for termination... (Ctrl+C to stop)&quot;);\n\n        query.awaitTermination();\n</code></pre>\n<p><a href=\"https://i.sstatic.net/eSgG6zvI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/eSgG6zvI.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}