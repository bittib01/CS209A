{
  "question": {
    "tags": [
      "python",
      "java",
      "pyspark"
    ],
    "owner": {
      "account_id": 38980730,
      "reputation": 1,
      "user_id": 29047749,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7f5f7ab1640275e8f7f6b971cc990929?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Teo Hui Fang",
      "link": "https://stackoverflow.com/users/29047749/teo-hui-fang"
    },
    "is_answered": false,
    "view_count": 330,
    "answer_count": 2,
    "score": 0,
    "last_activity_date": 1744719408,
    "creation_date": 1735981162,
    "question_id": 79328521,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79328521/windows-java-io-eofexception-error-when-trying-to-show-spark-dataframe",
    "title": "Windows java.io.EOFException error when trying to show spark dataframe",
    "body": "<p>Following is the code i have ran</p>\n<pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.master(&quot;local&quot;).appName(&quot;PySpark Installation Test&quot;).getOrCreate()\nspark.sparkContext.setLogLevel(&quot;DEBUG&quot;)\ndf = spark.createDataFrame([(1, &quot;Hello&quot;), (2, &quot;World&quot;)], [&quot;id&quot;, &quot;message&quot;])\ndf.show()\n</code></pre>\n<p>Kept on bumping into this issue:</p>\n<pre><code>---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\nCell In[4], line 2\n      1 df = spark.createDataFrame([(1, &quot;Hello&quot;), (2, &quot;World&quot;)], [&quot;id&quot;, &quot;message&quot;])\n----&gt; 2 df.show()\n\nFile c:\\Users\\hermo\\miniconda3\\envs\\nudgerank-venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947, in DataFrame.show(self, n, truncate, vertical)\n    887 def show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -&gt; None:\n    888     &quot;&quot;&quot;Prints the first ``n`` rows to the console.\n    889 \n    890     .. versionadded:: 1.3.0\n   (...)\n    945     name | Bob\n    946     &quot;&quot;&quot;\n--&gt; 947     print(self._show_string(n, truncate, vertical))\n\nFile c:\\Users\\hermo\\miniconda3\\envs\\nudgerank-venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965, in DataFrame._show_string(self, n, truncate, vertical)\n    959     raise PySparkTypeError(\n    960         error_class=&quot;NOT_BOOL&quot;,\n    961         message_parameters={&quot;arg_name&quot;: &quot;vertical&quot;, &quot;arg_type&quot;: type(vertical).__name__},\n    962     )\n    964 if isinstance(truncate, bool) and truncate:\n--&gt; 965     return self._jdf.showString(n, 20, vertical)\n    966 else:\n    967     try:\n\nFile c:\\Users\\hermo\\miniconda3\\envs\\nudgerank-venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, &quot;_detach&quot;):\n\nFile c:\\Users\\hermo\\miniconda3\\envs\\nudgerank-venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    177 def deco(*a: Any, **kw: Any) -&gt; Any:\n    178     try:\n--&gt; 179         return f(*a, **kw)\n    180     except Py4JJavaError as e:\n    181         converted = convert_exception(e.java_exception)\n\nFile c:\\Users\\hermo\\miniconda3\\envs\\nudgerank-venv\\Lib\\site-packages\\py4j\\protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\n    325 if answer[1] == REFERENCE_TYPE:\n--&gt; 326     raise Py4JJavaError(\n    327         &quot;An error occurred while calling {0}{1}{2}.\\n&quot;.\n    328         format(target_id, &quot;.&quot;, name), value)\n    329 else:\n    330     raise Py4JError(\n    331         &quot;An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n&quot;.\n    332         format(target_id, &quot;.&quot;, name, value))\n\nPy4JJavaError: An error occurred while calling o43.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:26)\n    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n    at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n    at org.apache.spark.scheduler.Task.run(Task.scala:141)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n    at java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.EOFException\n    at java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n    at java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n    ... 26 more\n\nDriver stacktrace:\n    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n    at scala.Option.foreach(Option.scala:407)\n    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n    at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n    at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n    at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n    at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n    at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n    at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n    at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n    at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n    at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n    at java.base/java.lang.reflect.Method.invoke(Method.java:580)\n    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n    at py4j.Gateway.invoke(Gateway.java:282)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(generated.java:26)\n    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n    at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n    at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n    at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n    at org.apache.spark.scheduler.Task.run(Task.scala:141)\n    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n    ... 1 more\nCaused by: java.io.EOFException\n    at java.base/java.io.DataInputStream.readFully(DataInputStream.java:210)\n    at java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\n    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n    ... 26 more\n</code></pre>\n<p>Need some help to solve this. I have added all environment variables following this guide: <a href=\"https://medium.com/@deepaksrawat1906/a-step-by-step-guide-to-installing-pyspark-on-windows-3589f0139a30\" rel=\"nofollow noreferrer\">https://medium.com/@deepaksrawat1906/a-step-by-step-guide-to-installing-pyspark-on-windows-3589f0139a30</a></p>\n<p>I am just trying to create and show a simple spark dataframe. I am using windows, tried reinstalling java, spark and hadoop still the same</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}