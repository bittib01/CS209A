{
  "question": {
    "tags": [
      "java",
      "apache-spark",
      "apache-spark-sql",
      "bigdata",
      "data-engineering"
    ],
    "owner": {
      "account_id": 44673027,
      "reputation": 1,
      "user_id": 31875949,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5b99f36517d45b9ee785aedbae674d4f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Parth Sarthi Roy",
      "link": "https://stackoverflow.com/users/31875949/parth-sarthi-roy"
    },
    "is_answered": true,
    "view_count": 95,
    "answer_count": 5,
    "score": 0,
    "last_activity_date": 1763731341,
    "creation_date": 1763100806,
    "question_id": 79819692,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79819692/pushing-down-filters-in-rdbms-with-java-spark",
    "title": "Pushing down filters in RDBMS with Java Spark",
    "body": "<p>I have been working as a <strong>Data Engineer</strong> and got this issue.<br />\nI came across a use case where I have a view(lets name it as inputView) which is created by reading data from some source.<br />\nNow somewhere later in the pipeline I have to again read data from RDBMS ,create a view(transactions).<br />\nThen I am running a Spark SQL query to join transactions view with input View based on some column.</p>\n<p><strong>Problem:</strong><br />\nThe problem here is that only the data which is required from transactions should be loaded but whole data is getting read.</p>\n<p><strong>Proposed Solution(Not Sure it is safe)</strong><br />\nTo solve this issue we are planning to create temp table in db and store inputView in RDBMS then do a join at db level and get data.<br />\nBut the issue seems with this approach that<br />\n1. Temp data is available per session only. Spark executors will have their separate session while inserting inputView data in db. So how are they going to read data after join as spark write api will create session write data and then closes session. Even before the join query the data will be gone.<br />\n2. If I write each record one by one from driver using JDBC prepareStatements. Then for doing join and reading data I have to use the same connection to read data, I can't use spark read api to read it. So I read data by JDBC only, which will eventually load all the data in the driver, that can cause OOM.<br />\n3. Suppose multiple pipelines are running and mulitple pipelines try to insert their inputView data in some temp table. The database will be getting a lot of load, Won't it crash ?</p>\n<p>Any Suggestion/Solution is welcomed.</p>\n<p>Thanks in advance.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}