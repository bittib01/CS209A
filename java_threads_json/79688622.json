{
  "question": {
    "tags": [
      "java",
      "jpa",
      "java-stream"
    ],
    "owner": {
      "account_id": 1073482,
      "reputation": 12781,
      "user_id": 1072187,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/6a15142de5577986384774484ff25fe6?s=256&d=identicon&r=PG",
      "display_name": "John Little",
      "link": "https://stackoverflow.com/users/1072187/john-little"
    },
    "is_answered": true,
    "view_count": 208,
    "answer_count": 2,
    "score": -3,
    "last_activity_date": 1764364563,
    "creation_date": 1751535502,
    "last_edit_date": 1764364563,
    "question_id": 79688622,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79688622/how-to-use-streams-to-process-chunks-in-jpa-stream-of-records",
    "title": "How to use streams to process chunks in JPA stream of records",
    "body": "<p>In Spring Boot 2.7, streaming results back from a database query and processing them in batches using JDBC is very easy:</p>\n<pre class=\"lang-java prettyprint-override\"><code>   try (PreparedStatement ps = conn.prepareStatement(MY_SQL)) {\n            ps.setFetchSize(100);\n              ResultSet rs = ps.executeQuery();\n         do {\n            List&lt;MyEntity&gt; chunk = new ArrayList&lt;&gt;();\n            while (rs.next() &amp;&amp; (rowsReadInThisBatch &lt; myBatchSize)) {\n                    MyEntity entity = new MyEntity();\n                    myEntity.setSomeCol(rs.getLong(&quot;some_col&quot;);\n                    myEntity.setSomeCol2(rs.getLong(&quot;some_col2&quot;);\n                    chunk.add(entity)\n                    rowsReadInThisBatch++;\n                }\n            // lost more code.\n            process(chunk);\n            writeBackToDB(cunk);\n            // lots more code\n        } while (rowsReadInThisBatch == myBatchSize);\n</code></pre>\n<p>The database table has &gt; 1 million rows, so we can't just read them all into a single 1 million row object as our production microservice servers only have 1 GB RAM. In the above example, we only ever have myBatchSize records in RAM, and we only need to do total/myBatchSize batch inserts to the database (not one insert for every record which would result in 1 million round trips to DB).</p>\n<p>To do this with streams in JPA, there is no equivalent of <code>rs.next()</code>. Instead you have to use lambdas and Java streams stuff.</p>\n<p>There is ForEach:</p>\n<pre class=\"lang-java prettyprint-override\"><code>    try(Stream&lt;MyEntity&gt; myStream = postRepository.streamByCreatedOnSince(yesterday)) {   \n        myStream.forEach(\n             ....\n        );\n    }\n</code></pre>\n<p>However, forEach is extremely limited because you can't use non-final variables to count how many have been processed etc.</p>\n<p>How do I convert my plain old <code>do</code> loops and while loops into Java's streams/lambda stuff in order to read in chunks and process chunks, and not process each one by one with no chunking? Ideally not using inline lambda code which is hard to test separately (and hard to understand for non-Java streams experts)?</p>\n<p>The crux is Java streams don't have the concept of a counter or chunking.</p>\n<p>This article: <a href=\"https://www.baeldung.com/java-stream-batch-processing\" rel=\"nofollow noreferrer\">https://www.baeldung.com/java-stream-batch-processing</a> has some very esoteric solutions requiring additional libraries which we want to avoid. Also, it's not clear if their solutions read the entire stream into memory then break it into chunks or batches, something we want to avoid due to the very large data set.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}