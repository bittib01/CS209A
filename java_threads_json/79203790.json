{
  "question": {
    "tags": [
      "python",
      "java",
      "windows",
      "apache-spark"
    ],
    "owner": {
      "account_id": 7432960,
      "reputation": 11,
      "user_id": 5653263,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d8e1438de87feecd7997b2f973cdf64c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alex Bridges",
      "link": "https://stackoverflow.com/users/5653263/alex-bridges"
    },
    "is_answered": false,
    "view_count": 422,
    "answer_count": 1,
    "score": 1,
    "last_activity_date": 1732974100,
    "creation_date": 1732025969,
    "last_edit_date": 1732026277,
    "question_id": 79203790,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79203790/apache-spark-java-net-socketexception-connection-reset-error-on-windows",
    "title": "Apache Spark: &quot;java.net.SocketException: Connection reset&quot; Error on Windows",
    "body": "<p>I'm trying to set up Apache Spark on Windows 10 but keep getting errors when running spark from VSCode:</p>\n<pre><code># Imports\nfrom pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder.appName('PySpark Sample DataFrame').getOrCreate()\n\n# Define Schema\ncol_schema = [&quot;Language&quot;, &quot;Version&quot;]\n\n# Prepare Data\nData = ((&quot;Jdk&quot;,&quot;17.0.12&quot;), (&quot;Python&quot;, &quot;3.11.9&quot;), (&quot;Spark&quot;, &quot;3.5.1&quot;),   \\\n    (&quot;Hadoop&quot;, &quot;3.3 and later&quot;), (&quot;Winutils&quot;, &quot;3.6&quot;),  \\\n  )\n\n# Create DataFrame\ndf = spark.createDataFrame(data = Data, schema = col_schema)\ndf.printSchema()\ndf.show(5,truncate=False)\n</code></pre>\n<p>. So far I've:</p>\n<ul>\n<li><p>Used to Python 3.11.8 <a href=\"https://stackoverflow.com/a/78157930/5653263\">https://stackoverflow.com/a/78157930/5653263</a></p>\n</li>\n<li><p>Downgraded to JDK 17: <a href=\"https://stackoverflow.com/a/76432417/5653263\">https://stackoverflow.com/a/76432417/5653263</a></p>\n</li>\n</ul>\n<p>Still I get the following error:</p>\n<pre><code>PS C:\\...&gt; &amp; c:/.../python.exe &quot;c:/.../test.py&quot;\nSetting default log level to &quot;WARN&quot;.\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/11/16 16:40:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nroot\n |-- Language: string (nullable = true)\n |-- Version: string (nullable = true) \n\n24/11/16 16:40:42 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]   \njava.net.SocketException: Connection reset\n        at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)     \n        at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)\n        at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)       \n        at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)       \n        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)     \n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n        at java.base/java.lang.Thread.run(Thread.java:842)\n24/11/16 16:40:42 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (Laptop executor driver): java.net.SocketException: Connection reset\n        at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)\n        at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)\n        at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)\n        at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n        at java.base/java.lang.Thread.run(Thread.java:842)\n\n24/11/16 16:40:42 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\nTraceback (most recent call last):\n  File &quot;c:\\...\\test.py&quot;, line 18, in &lt;module&gt;\n    df.show(5,truncate=False)\n  File &quot;C:\\...\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py&quot;, line 947, in show\n    print(self._show_string(n, truncate, vertical))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;C:\\...\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py&quot;, line 978, in _show_string\n    return self._jdf.showString(n, int_truncate, vertical)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File &quot;C:\\...\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py&quot;, line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File &quot;C:\\...\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py&quot;, line 179, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File &quot;C:\\...\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py&quot;, line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o42.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (Laptop executor driver): java.net.SocketException: Connection reset\n        at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)\n        at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)\n        at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)\n        at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n        at java.base/java.lang.Thread.run(Thread.java:842)\n\nDriver stacktrace:\n        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n        at scala.Option.foreach(Option.scala:407)\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n        at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n        at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n        at org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n        at org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n        at org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n        at org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:568)\n        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n        at py4j.Gateway.invoke(Gateway.java:282)\n        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n        at py4j.commands.CallCommand.execute(CallCommand.java:79)\n        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n        at java.base/java.lang.Thread.run(Thread.java:842)\nCaused by: java.net.SocketException: Connection reset\n        at java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:328)\n        at java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:355)\n        at java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:808)\n        at java.base/java.net.Socket$SocketInputStream.read(Socket.java:966)\n        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\n        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\n        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\n        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n        at org.apache.spark.scheduler.Task.run(Task.scala:141)\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n        ... 1 more\n\nPS C:\\...\\&gt; SUCCESS: The process with PID 24468 (child process of PID 3840) has been terminated.\nSUCCESS: The process with PID 3840 (child process of PID 29208) has been terminated.\nSUCCESS: The process with PID 29208 (child process of PID 29332) has been terminated.\n</code></pre>\n<p>Software Versions:</p>\n<ul>\n<li><p>Windows: 10.0.19045.5011 x64</p>\n</li>\n<li><p>JDK: 17.0.12</p>\n</li>\n<li><p>Python: 3.11.8</p>\n</li>\n<li><p>Spark: 3.5.3</p>\n</li>\n<li><p>Hadoop (Winutils): 3.3.6 (<a href=\"https://github.com/cdarlint/winutils/tree/master/hadoop-3.3.6/bin\" rel=\"nofollow noreferrer\">https://github.com/cdarlint/winutils/tree/master/hadoop-3.3.6/bin</a>)</p>\n</li>\n</ul>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}