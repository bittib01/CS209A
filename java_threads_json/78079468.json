{
  "question": {
    "tags": [
      "java",
      "java-11",
      "jit"
    ],
    "owner": {
      "account_id": 5398341,
      "reputation": 163,
      "user_id": 8192138,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/61f240c78def7a5af9084b26e7a332cf?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "onRiv",
      "link": "https://stackoverflow.com/users/8192138/onriv"
    },
    "is_answered": false,
    "view_count": 79,
    "answer_count": 1,
    "score": 0,
    "last_activity_date": 1715409513,
    "creation_date": 1709190837,
    "question_id": 78079468,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/78079468/suspiciously-large-mmap-in-c2compilercompile-method-phasecfginsert-anti-depe",
    "title": "suspiciously large mmap in C2Compiler::compile_method/PhaseCFG::insert_anti_dependences",
    "body": "<p>We are suspecting that some peak allocation behavior of C2Compiler causes the memory used by java reach a peak memory exceeding the limit of docker and invokes the oom killer:</p>\n<p>For continuing checking the docker oom occurred in our java web service hosted inside a docker container with 32GB memory limit, with version</p>\n<pre><code>openjdk version &quot;11.0.14.1&quot; 2022-02-08\nOpenJDK Runtime Environment Temurin-11.0.14.1+1 (build 11.0.14.1+1)\nOpenJDK 64-Bit Server VM Temurin-11.0.14.1+1 (build 11.0.14.1+1, mixed mode)\n</code></pre>\n<p>with the following java options:</p>\n<pre><code>-Xms28g\n-Xmx28g\n-Xss256k\n-XX:+UseG1GC\n-XX:MaxGCPauseMillis=200\n-XX:ParallelGCThreads=12\n-XX:MetaspaceSize=512m\n-XX:MaxMetaspaceSize=512m\n-XX:InitialCodeCacheSize=128m\n-XX:ReservedCodeCacheSize=512m\n-XX:MinHeapFreeRatio=30\n-XX:MaxHeapFreeRatio=50\n-XX:CICompilerCount=4\n-XX:+UseCompressedOops\n-XX:SoftRefLRUPolicyMSPerMB=0\n-XX:-OmitStackTraceInFastThrow\n-XX:+AggressiveOpts\n-XX:+PrintGC\n-XX:+PrintGCDetails\n-XX:+PrintClassHistogram\n-XX:MaxTenuringThreshold=10\n-XX:+IgnoreUnrecognizedVMOptions\n-Djava.lang.Integer.IntegerCache.high=1000000\n-Dcustomer.java.lang.Integer.IntegerCache.high=1000000\n-Djava.util.concurrent.ForkJoinPool.common.parallelism=8\n-Djdk.attach.allowAttachSelf=true\n-Dfastjson.parser.safeMode=true\n-Dlog4j2.formatMsgNoLookups=true\n-Dio.netty.allocator.numDirectArenas=8\n</code></pre>\n<p>For checking this in a larger memory limit environment, using strace we found that there is a continous Arena growth during the following stack trace of C2Compiler::compile_method like:</p>\n<pre><code>[pid 24629] 20:40:44 mmap(NULL, 335544320, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c35378000\n&gt; /usr/lib64/libc-2.17.so(mmap64+0x3a) [0xf8fca]\n&gt; /opt/container/lib/libjemalloc.so(je_pages_map+0x47) [0x50657]\n&gt; /opt/container/lib/libjemalloc.so(je_extent_alloc_mmap+0x13) [0x4ab23]\n&gt; /opt/container/lib/libjemalloc.so(extent_grow_retained+0x709) [0x48ed9]\n&gt; /opt/container/lib/libjemalloc.so(je_extent_alloc_wrapper+0x5bf) [0x4997f]\n&gt; /opt/container/lib/libjemalloc.so(je_arena_extent_alloc_large+0x173) [0x20123]\n&gt; /opt/container/lib/libjemalloc.so(je_large_malloc+0xb9) [0x4b739]\n&gt; /opt/container/lib/libjemalloc.so(je_malloc_default+0x6c3) [0xf3f3]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(os::malloc(unsigned long, MemoryType, NativeCallStack const&amp;)+0xfc) [0xc5edec]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(Arena::grow(unsigned long, AllocFailStrategy::AllocFailEnum)+0x109) [0x44d1f9]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(Arena::Arealloc(void*, unsigned long, unsigned long, AllocFailStrategy::AllocFailEnum)+0x201) [0x44d541]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(Node_Array::grow(unsigned int)+0x56) [0xc348a6]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(PhaseCFG::insert_anti_dependences(Block*, Node*, bool)+0x750) [0x827160]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(PhaseCFG::schedule_late(VectorSet&amp;, Node_Stack&amp;)+0x447) [0x82a0c7]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(PhaseCFG::global_code_motion()+0x314) [0x82d564]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(PhaseCFG::do_global_code_motion()+0x49) [0x82dea9]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(Compile::Code_Gen()+0x1d4) [0x66cc64]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(Compile::Compile(ciEnv*, C2Compiler*, ciMethod*, int, bool, bool, bool, bool, DirectiveSet*)+0xd4a) [0x6704da]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(C2Compiler::compile_method(ciEnv*, ciMethod*, int, DirectiveSet*)+0xd3) [0x588193]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(CompileBroker::invoke_compiler_on_method(CompileTask*)+0x445) [0x67a8c5]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(CompileBroker::compiler_thread_loop()+0x5a7) [0x67c1f7]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(JavaThread::thread_main_inner()+0x1b9) [0xed4899]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(Thread::call_run()+0x14e) [0xed149e]\n&gt; /usr/java/jdk11/lib/server/libjvm.so(thread_native_entry(Thread*)+0xed) [0xc714cd]\n&gt; /usr/lib64/libpthread-2.17.so(start_thread+0xc4) [0x7ea4]\n&gt; /usr/lib64/libc-2.17.so(__clone+0x6c) [0xfeb0c]\n</code></pre>\n<p>the strace of mmap shows that (all with the above stack trace) that sums up to more than 1GB memory:</p>\n<pre><code>[pid 24629] 20:40:33 mmap(NULL, 29360128, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5ca6678000\n[pid 24629] 20:40:35 mmap(NULL, 50331648, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c9ee78000\n[pid 24629] 20:40:36 mmap(NULL, 58720256, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c9b678000\n[pid 24629] 20:40:36 mmap(NULL, 67108864, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c97678000\n[pid 24629] 20:40:36 mmap(NULL, 83886080, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c92678000\n[pid 24629] 20:40:39 mmap(NULL, 100663296, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c8c678000\n[pid 24629] 20:40:41 mmap(NULL, 134217728, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c7d678000\n[pid 24629] 20:40:42 mmap(NULL, 167772160, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c73678000\n[pid 24629] 20:40:42 mmap(NULL, 201326592, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c67678000\n[pid 24629] 20:40:43 mmap(NULL, 234881024, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c59378000\n[pid 24629] 20:40:43 mmap(NULL, 268435456, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c49378000\n[pid 24629] 20:40:44 mmap(NULL, 335544320, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0) = 0x7f5c35378000\n</code></pre>\n<p>And in /proc/$pid/status we see that <a href=\"https://stackoverflow.com/questions/774556/peak-memory-usage-of-a-linux-unix-process\">the metric VmHWM</a>  from 31.04GB to 32.12GB at the same time, which means at some moment the process used memory at much 32.12GB. But in the NativeMemoryTool we only sample a record with java.nmt.compiler.committed=122MB.\nWith -XX:+PrintCompilation option it seems no too much methods being compiled (20:40:33 to 20:40:44 is of range seconds 1937 to 1948, names of some packages replaced)</p>\n<pre><code>1936335 58263       4       com.wecorp.webiz.product.service.v1.PublicParametersEntity::put (4606 bytes)\n1936376 52134       3       org.apache.catalina.valves.AbstractAccessLogValve$RequestURIElement::addElement (24 bytes)   made not entrant\n1936507 58259       3       com.wecorp.webiz.facade.mapper.PublicParametersEntityMapper::mapToInternal (6527 bytes)\n1936634 58262       4       com.wecorp.webiz.cache.DataBusinessV2::getHash (143 bytes)\n1936986 56014       3       com.wecorp.webiz.cache.DataBusinessV2::getHash (143 bytes)   made not entrant\n1937112 58251       4       java.util.IdentityHashMap::containsKey (55 bytes)\n1937141  898       3       java.util.IdentityHashMap::containsKey (55 bytes)   made not entrant\n1937288 58264       4       com.wecorp.webiz.business.dynamic.simplefilter.Filter$$Lambda$1463/0x0000000800b54040::apply (13 bytes)\n1937334 36436       2       com.wecorp.webiz.business.dynamic.simplefilter.Filter$$Lambda$1463/0x0000000800b54040::apply (13 bytes)   made not entrant\n1938478 58267       4       com.wecorp.webiz.facade.mapper.WebizFacilitiesEntityMapper::mapToInternal (1541 bytes)\n1938588 52139   !   3       org.apache.catalina.core.StandardHostValve::invoke (402 bytes)   made not entrant\n1938739 58272   !   4       io.grpc.internal.MessageDeframer::readRequiredBytes (558 bytes)\n1939036 53988       4       com.wecorp.webiz.cache.BaseInfoBusinessV10::getByBaseId (109 bytes)   made not entrant\n1942257 23486   !   3       io.grpc.internal.MessageDeframer::readRequiredBytes (558 bytes)   made not entrant\n1942481 58276       4       com.wecorp.webiz.business.plan.ResourceTag::getSpecialTypes (552 bytes)\n1942733 47364       3       com.wecorp.webiz.facade.mapper.WebizFacilitiesEntityMapper::mapToInternal (1541 bytes)   made not entrant\n1942768 58166       3       com.wecorp.webiz.business.plan.ResourceTag::getSpecialTypes (552 bytes)   made not entrant\n1943829 58278       4       com.wecorp.webiz.business.plan.model.hour.HourStgy::isHour (14 bytes)\n1943841 58282       4       com.wecorp.webiz.business.plan.model.hour.CodeHour::isHour (73 bytes)\n1944863 58167       3       com.wecorp.webiz.business.plan.model.hour.CodeHour::isHour (73 bytes)   made not entrant\n1944928 58168       3       com.wecorp.webiz.business.plan.model.hour.HourStgy::isHour (14 bytes)   made not entrant\n1945127 58271       4       com.wecorp.webiz.business.factory.lazy.LazyToWebizVMS::isSupport (39 bytes)\n1945134 46559       3       com.wecorp.webiz.business.factory.lazy.LazyToWebizVMS::isSupport (39 bytes)   made not entrant\n1946397 58269       4       io.grpc.Deadline$SystemTicker::nanoTime (4 bytes)\n1946418 23483       3       io.grpc.Deadline$SystemTicker::nanoTime (4 bytes)   made not entrant\n1947083 58295       4       com.wecorp.infra.kafka.common.protocol.types.Struct::instance (13 bytes)\n1947448 54974       4       com.wecorp.webiz.business.dynamic.webizfilterrules.FilterSpecialWebiz::filter (174 bytes)   made not entrant\n1949042 40631       3       com.wecorp.infra.kafka.common.protocol.types.Struct::instance (13 bytes)   made not entrant\n1949471 58297       4       com.wecorp.webiz.business.dynamic.webizfilterrules.FilterSpecialWebiz::filter (174 bytes)\n1949474 58306       4       com.wecorp.webiz.product.cache.Cache::isDataReady (26 bytes)\n1949479 27680       3       com.wecorp.webiz.product.cache.Cache::isDataReady (26 bytes)   made not entrant\n</code></pre>\n<p>May I ask why there is a continuous peak alloc in C2Compiler::compile_method, namely in PhaseCFG::insert_anti_dependences? it seems that the code of <a href=\"https://github.com/openjdk/jdk11u/blob/master/src/hotspot/share/memory/arena.cpp#L353\" rel=\"nofollow noreferrer\">Arena::grow</a> does not free each chunk? It means that at some point all the allocated memory is using?\nAnd can we avoid this kind of continuously large size allocation in this limited memory environment in docker but still enable C2Compiler?</p>\n<p>For solving the docker oom, we tried continuing metric with native memory tool, and many tools like pmap or strace. Now we suspect that a peak alloc in C2Compiler::compile_method of JDK11u and we want to understand the behavior and solve it.</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}