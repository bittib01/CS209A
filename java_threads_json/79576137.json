{
  "question": {
    "tags": [
      "java",
      "time",
      "clock",
      "system-clock",
      "current-time"
    ],
    "owner": {
      "account_id": 1856144,
      "reputation": 10346,
      "user_id": 1681681,
      "user_type": "registered",
      "accept_rate": 85,
      "profile_image": "https://www.gravatar.com/avatar/626b7e8b46910cd41920bf22d35d119e?s=256&d=identicon&r=PG",
      "display_name": "mchen",
      "link": "https://stackoverflow.com/users/1681681/mchen"
    },
    "is_answered": false,
    "view_count": 288,
    "answer_count": 1,
    "score": 3,
    "last_activity_date": 1745973804,
    "creation_date": 1744757046,
    "last_edit_date": 1744760835,
    "question_id": 79576137,
    "content_license": "CC BY-SA 4.0",
    "link": "https://stackoverflow.com/questions/79576137/why-does-instant-now-have-a-much-higher-precision-on-my-unix-machine-vs-window",
    "title": "Why does Instant.now() have a much higher precision on my Unix machine vs Windows?",
    "body": "<p>To get a sense of how accurate <code>Instant.now()</code> is on various machines, I ran a very simple test to see how often the clock updated:</p>\n<pre><code>public class Test {\n    private static final Logger logger = LogManager.getLogger(Test.class.getName());\n\n    public static void main(String... args) {\n        while (true) {\n            logger.info(Instant.now());\n        }\n    }\n}\n</code></pre>\n<p>On a Intel Core i9 laptop running Windows 11, most of the log lines printed the same timestamp, and would only change every 1ms or so.</p>\n<pre><code>18:23:55.325 [main] INFO Test - 2025-04-15T22:23:55.325858100Z\n18:23:55.325 [main] INFO Test - 2025-04-15T22:23:55.325858100Z\n18:23:55.325 [main] INFO Test - 2025-04-15T22:23:55.325858100Z\n18:23:55.325 [main] INFO Test - 2025-04-15T22:23:55.325858100Z\n18:23:55.325 [main] INFO Test - 2025-04-15T22:23:55.325858100Z\n18:23:55.326 [main] INFO Test - 2025-04-15T22:23:55.326858800Z\n18:23:55.326 [main] INFO Test - 2025-04-15T22:23:55.326858800Z\n18:23:55.326 [main] INFO Test - 2025-04-15T22:23:55.326858800Z\n18:23:55.326 [main] INFO Test - 2025-04-15T22:23:55.326858800Z\n18:23:55.326 [main] INFO Test - 2025-04-15T22:23:55.326858800Z\n</code></pre>\n<p>On a Dell T620 server running Debian 12 (with chrony NTP), every timestamp was different and increasing (maybe 5-10us apart).</p>\n<pre><code>18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585059578Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585065991Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585072460Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585078943Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585085285Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585091618Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585113372Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585122554Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585129166Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585135690Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585142432Z\n18:18:04.585 [main] INFO Test - 2025-04-15T22:18:04.585148890Z\n</code></pre>\n<p>Both machines used Java 21.</p>\n<p>I'm just curious why one machine can measure times with microsecond precision while the other can only do so with millisecond precision. That's not just a little less precision - it's a whole order of magnitude less precision.</p>\n<p>Microsecond precision starts becoming important when you're comparing timestamps produced by different processes/machines, and so cannot rely on <code>System.nanoTime()</code>.</p>\n<p>Is it a Windows vs Unix thing? Or is it more likely there's fundamentally better hardware clocks in the server?</p>\n"
  },
  "answers": [],
  "question_comments": [],
  "answer_comments": {}
}